{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymystem3 -q\n",
    "# !pip install transformers -q\n",
    "# !pip install emoji -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "import logging\n",
    "import warnings\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Научные и аналитические библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Библиотеки для бустинга\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Работа с текстом\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from pymystem3 import Mystem\n",
    "import emoji\n",
    "\n",
    "# PyTorch и Transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import transformers\n",
    "\n",
    "# Оптимизация гиперпараметров\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Прочие полезные инструменты\n",
    "from tqdm import tqdm, notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Отключение предупреждений\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BERT ---\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "# stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "english_stopwords = nltk_stopwords.words(\"english\")\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# torch.compile — ТОЛЬКО если поддерживается\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        logger.info(\"torch.compile активен\")\n",
    "    except RuntimeError as e:\n",
    "        logger.warning(f\"torch.compile отключен: {e}\")\n",
    "\n",
    "logger.info(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 64\n",
    "CV = 3\n",
    "N_OPTUNA = 100\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 20\n",
    "N_EPOCHS = 10000\n",
    "EARLY_STOP = 10\n",
    "\n",
    "def seed_everything(seed=RANDOM_STATE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', index_col=[0])\n",
    "df = pd.read_csv('./data/toxic_comments.csv', index_col=[0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "corpus = df['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Очистка текста для BERT:\n",
    "    - убираем ссылки,\n",
    "    - заменяем несколько пробелов на один,\n",
    "    - убираем эмодзи,\n",
    "    - приводим к нижнему регистру,\n",
    "    - обрезаем лишние пробелы по краям.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # убираем ссылки\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # заменяем несколько пробелов на один\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # убираем эмодзи\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# применяем к датафрейму\n",
    "df['text_clean'] = df['text'].progress_apply(bert_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"toxic\"]\n",
    ")\n",
    "\n",
    "# сброс индексов\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_mean_embeddings(df_part, tokenizer, model, device, batch_size, max_length, desc):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df_part), batch_size), desc=desc):\n",
    "        batch_texts = df_part['text_clean'].iloc[i:i + batch_size].tolist()\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "\n",
    "        embeddings = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пути для сохранения эмбеддингов\n",
    "train_emb_path = \"./data/X_train_emb.npy\"\n",
    "test_emb_path = \"./data/X_test_emb.npy\"\n",
    "train_labels_path = \"./data/y_train.npy\"\n",
    "test_labels_path = \"./data/y_test.npy\"\n",
    "\n",
    "# ===== TRAIN =====\n",
    "if os.path.exists(train_emb_path) and os.path.exists(train_labels_path):\n",
    "    X_train = np.load(train_emb_path)\n",
    "    y_train = np.load(train_labels_path)\n",
    "    logger.info(f\"Загружены кэшированные эмбеддинги TRAIN: {X_train.shape}\")\n",
    "else:\n",
    "    X_train = bert_mean_embeddings(\n",
    "        df_train,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        device,\n",
    "        BATCH_SIZE,\n",
    "        MAX_LENGTH,\n",
    "        desc=\"Train embeddings\"\n",
    "    )\n",
    "    y_train = df_train[\"toxic\"].values\n",
    "    np.save(train_emb_path, X_train)\n",
    "    np.save(train_labels_path, y_train)\n",
    "    logger.info(f\"Сгенерированы и сохранены эмбеддинги TRAIN: {X_train.shape}\")\n",
    "\n",
    "# ===== TEST =====\n",
    "if os.path.exists(test_emb_path) and os.path.exists(test_labels_path):\n",
    "    X_test = np.load(test_emb_path)\n",
    "    y_test = np.load(test_labels_path)\n",
    "    logger.info(f\"Загружены кэшированные эмбеддинги TEST: {X_test.shape}\")\n",
    "else:\n",
    "    X_test = bert_mean_embeddings(\n",
    "        df_test,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        device,\n",
    "        BATCH_SIZE,\n",
    "        MAX_LENGTH,\n",
    "        desc=\"Test embeddings\"\n",
    "    )\n",
    "    y_test = df_test[\"toxic\"].values\n",
    "    np.save(test_emb_path, X_test)\n",
    "    np.save(test_labels_path, y_test)\n",
    "    logger.info(f\"Сгенерированы и сохранены эмбеддинги TEST: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "# пробовал рандомный лес, дерево решений - дольше и хуже, XGB тоже долго но результат значительно лучше\n",
    "# models_to_run = [\"LogisticRegression\", \"RidgeClassifier\", \"LGBM\", \"XGB\"]\n",
    "models_to_run = [\"LogisticRegression\", \"RidgeClassifier\"]\n",
    "\n",
    "for model_name in models_to_run:\n",
    "\n",
    "    def objective(trial):\n",
    "        # --- выбираем модель и параметры ---\n",
    "        if model_name == \"LogisticRegression\":\n",
    "            C = trial.suggest_float(\"C\", 1e-3, 10.0, log=True)\n",
    "            clf = LogisticRegression(C=C, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "        elif model_name == \"RidgeClassifier\":\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "            clf = RidgeClassifier(alpha=alpha, class_weight=\"balanced\")\n",
    "        elif model_name == \"LGBM\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "            clf = LGBMClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                class_weight=\"balanced\",\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "        elif model_name == \"XGB\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "            clf = XGBClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric=\"logloss\",\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )\n",
    "\n",
    "        # --- кросс-валидация с прунингом ---\n",
    "        cv = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RANDOM_STATE)\n",
    "        f1_scores = []\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_pred = clf.predict(X_val)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            # прунинг: если среднее по текущим фолдам хуже медианы, отбрасываем trial\n",
    "            intermediate_value = np.mean(f1_scores)\n",
    "            trial.report(intermediate_value, step=fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        logger.info(f\"{model_name}: Trial {trial.number}, средний F1 = {mean_f1:.4f}\")\n",
    "        return mean_f1\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1),\n",
    "        sampler=TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=N_OPTUNA)\n",
    "\n",
    "    best_models[model_name] = {\n",
    "        \"best_trial\": study.best_trial,\n",
    "        \"best_value\": study.best_value\n",
    "    }\n",
    "\n",
    "# --- выбираем лучшую модель среди всех ---\n",
    "best_model_name = max(best_models, key=lambda k: best_models[k][\"best_value\"])\n",
    "best_trial = best_models[best_model_name][\"best_trial\"]\n",
    "params = best_trial.params\n",
    "\n",
    "# --- финальная модель ---\n",
    "if best_model_name == \"LogisticRegression\":\n",
    "    final_model = LogisticRegression(**params, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "elif best_model_name == \"RidgeClassifier\":\n",
    "    final_model = RidgeClassifier(**params, class_weight=\"balanced\")\n",
    "elif best_model_name == \"LGBM\":\n",
    "    final_model = LGBMClassifier(**params, class_weight=\"balanced\", n_jobs=-1, verbose=-1)\n",
    "elif best_model_name == \"XGB\":\n",
    "    final_model = XGBClassifier(**params, use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1, verbosity=0)\n",
    "\n",
    "# обучение на всем трейне\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# оценка на тесте\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "logger.info(f\"Лучшая модель: {best_model_name}\")\n",
    "logger.info(f\"Параметры: {params}\")\n",
    "logger.info(f\"F1 на test: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нейросеть\n",
    "# --- данные для CV (ТОЛЬКО train) ---\n",
    "X_cv = X_train.astype(np.float32)\n",
    "y_cv = y_train.astype(np.int64)\n",
    "\n",
    "# --- простая MLP ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim1, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- обучение одной модели с ранней остановкой ---\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    lr,\n",
    "    batch_size,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    patience=EARLY_STOP\n",
    "):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ВАЖНО: веса считаем ТОЛЬКО по train части\n",
    "    y_train_local = train_dataset.tensors[1].numpy()\n",
    "    class_counts = np.bincount(y_train_local)\n",
    "    class_weights = class_counts.sum() / (2 * class_counts)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    trigger = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # --- validation ---\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(yb.cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = model.state_dict()\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_val_f1\n",
    "\n",
    "# --- Optuna objective с CV (БЕЗ УТЕЧЕК) ---\n",
    "def objective_mlp(trial):\n",
    "    hidden_dim1 = trial.suggest_int(\"hidden_dim1\", 512, 1024)\n",
    "    hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 256, 512)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.15)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_cv, y_cv):\n",
    "        train_ds = TensorDataset(\n",
    "            torch.from_numpy(X_cv[train_idx]),\n",
    "            torch.from_numpy(y_cv[train_idx])\n",
    "        )\n",
    "        val_ds = TensorDataset(\n",
    "            torch.from_numpy(X_cv[val_idx]),\n",
    "            torch.from_numpy(y_cv[val_idx])\n",
    "        )\n",
    "\n",
    "        model = MLP(\n",
    "            input_dim=X_cv.shape[1],\n",
    "            hidden_dim1=hidden_dim1,\n",
    "            hidden_dim2=hidden_dim2,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        _, val_f1 = train_model(\n",
    "            model,\n",
    "            train_ds,\n",
    "            val_ds,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        f1_scores.append(val_f1)\n",
    "\n",
    "    return float(np.mean(f1_scores))\n",
    "\n",
    "# --- запуск Optuna ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_mlp, n_trials=N_OPTUNA)\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "logger.info(f\"Лучшие параметры: {best_params}\")\n",
    "logger.info(f\"F1 CV (train): {study.best_value:.4f}\")\n",
    "\n",
    "# --- финальная модель: обучение НА ВСЁМ train ---\n",
    "final_model = MLP(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim1=best_params[\"hidden_dim1\"],\n",
    "    hidden_dim2=best_params[\"hidden_dim2\"],\n",
    "    dropout=best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "full_train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train.astype(np.float32)),\n",
    "    torch.from_numpy(y_train.astype(np.int64))\n",
    ")\n",
    "\n",
    "# используем train как val только для early stopping\n",
    "final_model, _ = train_model(\n",
    "    final_model,\n",
    "    full_train_dataset,\n",
    "    full_train_dataset,\n",
    "    lr=best_params[\"lr\"],\n",
    "    batch_size=best_params[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# --- оценка на test (holdout) ---\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test.astype(np.float32)),\n",
    "    torch.from_numpy(y_test.astype(np.int64))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "final_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = final_model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(yb.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "test_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "logger.info(f\"F1 test: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
