{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymystem3 -q\n",
    "# !pip install transformers -q\n",
    "# !pip install emoji -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Стандартная библиотека\n",
    "import logging\n",
    "import warnings\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Научные и аналитические библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Библиотеки для бустинга\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Работа с текстом\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from pymystem3 import Mystem\n",
    "import emoji\n",
    "\n",
    "# PyTorch и Transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import transformers\n",
    "\n",
    "# Оптимизация гиперпараметров\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Прочие полезные инструменты\n",
    "from tqdm import tqdm, notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Отключение предупреждений\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2026-01-07 20:07:56 [WARNING] __main__: torch.compile отключен: torch.compile is not supported on Python 3.14+\n",
      "2026-01-07 20:07:56 [INFO] __main__: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- BERT ---\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "# stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "english_stopwords = nltk_stopwords.words(\"english\")\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# torch.compile — ТОЛЬКО если поддерживается\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        logger.info(\"torch.compile активен\")\n",
    "    except RuntimeError as e:\n",
    "        logger.warning(f\"torch.compile отключен: {e}\")\n",
    "\n",
    "logger.info(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 64\n",
    "CV = 3\n",
    "N_OPTUNA = 10\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 20\n",
    "N_EPOCHS = 10000\n",
    "EARLY_STOP = 10\n",
    "\n",
    "def seed_everything(seed=RANDOM_STATE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "2768c51e-7323-4838-879d-bc7d1d3fcccf",
       "rows": [
        [
         "0",
         "Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27",
         "0"
        ],
        [
         "1",
         "D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)",
         "0"
        ],
        [
         "2",
         "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.",
         "0"
        ],
        [
         "3",
         "\"\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"",
         "0"
        ],
        [
         "4",
         "You, sir, are my hero. Any chance you remember what page that's on?",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', index_col=[0])\n",
    "df = pd.read_csv('./data/toxic_comments.csv', index_col=[0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "corpus = df['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:42<00:00, 3780.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def bert_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Очистка текста для BERT:\n",
    "    - убираем ссылки,\n",
    "    - заменяем несколько пробелов на один,\n",
    "    - убираем эмодзи,\n",
    "    - приводим к нижнему регистру,\n",
    "    - обрезаем лишние пробелы по краям.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # убираем ссылки\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # заменяем несколько пробелов на один\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # убираем эмодзи\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# применяем к датафрейму\n",
    "df['text_clean'] = df['text'].progress_apply(bert_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_clean",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "33c34b1e-aef6-49bc-a46c-4d69c9d7c5f7",
       "rows": [
        [
         "74297",
         "Adoption?\nHi, Katie. I am also interested in adopting you. I have been a member for a long time, and edit many pages in German, French, and English. I am 3rd generation American, decending from Austrian/German heritage. Are you still interested in adoption? I would love to help you.",
         "0",
         "adoption? hi, katie. i am also interested in adopting you. i have been a member for a long time, and edit many pages in german, french, and english. i am 3rd generation american, decending from austrian/german heritage. are you still interested in adoption? i would love to help you."
        ],
        [
         "53437",
         "\"\n\n WP:APPLE's Backlog Elimination Drive is over! \n\nAs the coordinator of the WikiProject Apple Inc. Backlog Elimination Drive for May 2010, I would like to thank you for your active participation in the drive. \n\nI am writing to inform you that we have assessed all project articles.  We thank all of our participants, as we couldn't have done it without you!\n\nOnce again, thank you for participating, and we appreciate the meaningful drop in the numbers due to your hard work and efforts.  Awards will be delivered soon to our participants!\n\nCheers, mono \"",
         "0",
         "\" wp:apple's backlog elimination drive is over! as the coordinator of the wikiproject apple inc. backlog elimination drive for may 2010, i would like to thank you for your active participation in the drive. i am writing to inform you that we have assessed all project articles. we thank all of our participants, as we couldn't have done it without you! once again, thank you for participating, and we appreciate the meaningful drop in the numbers due to your hard work and efforts. awards will be delivered soon to our participants! cheers, mono \""
        ],
        [
         "68821",
         "Your recent removal of some text at Global Positioning System\n\nWas hilarious. Thank you. I snorted tea out my nose. ♠♠ (talk)",
         "0",
         "your recent removal of some text at global positioning system was hilarious. thank you. i snorted tea out my nose.  (talk)"
        ],
        [
         "14003",
         "No, we don't need anyone to say explicitly that the dubious genera don't exist; we just need a comprehensive listing of the genera that are present. At the moment, the only one of those post-dating the original description is WoRMS. Once it fixes its error, everything will be fine, but we can't lead the way. We can choose as editors to disregard WoRMS, which would involve removing the whole last paragraph. No-one has argued for that yet, but I could see it working. I'll give it a go.",
         "0",
         "no, we don't need anyone to say explicitly that the dubious genera don't exist; we just need a comprehensive listing of the genera that are present. at the moment, the only one of those post-dating the original description is worms. once it fixes its error, everything will be fine, but we can't lead the way. we can choose as editors to disregard worms, which would involve removing the whole last paragraph. no-one has argued for that yet, but i could see it working. i'll give it a go."
        ],
        [
         "155348",
         "Piss off, she is an ignorant bitch.",
         "1",
         "piss off, she is an ignorant bitch."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74297</th>\n",
       "      <td>Adoption?\\nHi, Katie. I am also interested in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>adoption? hi, katie. i am also interested in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53437</th>\n",
       "      <td>\"\\n\\n WP:APPLE's Backlog Elimination Drive is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\" wp:apple's backlog elimination drive is over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68821</th>\n",
       "      <td>Your recent removal of some text at Global Pos...</td>\n",
       "      <td>0</td>\n",
       "      <td>your recent removal of some text at global pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003</th>\n",
       "      <td>No, we don't need anyone to say explicitly tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, we don't need anyone to say explicitly tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155348</th>\n",
       "      <td>Piss off, she is an ignorant bitch.</td>\n",
       "      <td>1</td>\n",
       "      <td>piss off, she is an ignorant bitch.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "74297   Adoption?\\nHi, Katie. I am also interested in ...      0   \n",
       "53437   \"\\n\\n WP:APPLE's Backlog Elimination Drive is ...      0   \n",
       "68821   Your recent removal of some text at Global Pos...      0   \n",
       "14003   No, we don't need anyone to say explicitly tha...      0   \n",
       "155348                Piss off, she is an ignorant bitch.      1   \n",
       "\n",
       "                                               text_clean  \n",
       "74297   adoption? hi, katie. i am also interested in a...  \n",
       "53437   \" wp:apple's backlog elimination drive is over...  \n",
       "68821   your recent removal of some text at global pos...  \n",
       "14003   no, we don't need anyone to say explicitly tha...  \n",
       "155348                piss off, she is an ignorant bitch.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 20:08:40 [INFO] __main__: Train size: 127433, Test size: 31859\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"toxic\"]\n",
    ")\n",
    "\n",
    "# сброс индексов\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_mean_embeddings(df_part, tokenizer, model, device, batch_size, max_length, desc):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df_part), batch_size), desc=desc):\n",
    "        batch_texts = df_part['text_clean'].iloc[i:i + batch_size].tolist()\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "\n",
    "        embeddings = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train embeddings: 100%|██████████| 1992/1992 [09:03<00:00,  3.66it/s]\n",
      "2026-01-07 20:18:01 [INFO] __main__: Сгенерированы и сохранены эмбеддинги TRAIN: (127433, 768)\n",
      "Test embeddings: 100%|██████████| 498/498 [02:12<00:00,  3.75it/s]\n",
      "2026-01-07 20:20:14 [INFO] __main__: Сгенерированы и сохранены эмбеддинги TEST: (31859, 768)\n"
     ]
    }
   ],
   "source": [
    "# пути для сохранения эмбеддингов\n",
    "train_emb_path = \"./data/X_train_emb.npy\"\n",
    "test_emb_path = \"./data/X_test_emb.npy\"\n",
    "train_labels_path = \"./data/y_train.npy\"\n",
    "test_labels_path = \"./data/y_test.npy\"\n",
    "\n",
    "# ===== TRAIN =====\n",
    "if os.path.exists(train_emb_path) and os.path.exists(train_labels_path):\n",
    "    X_train = np.load(train_emb_path)\n",
    "    y_train = np.load(train_labels_path)\n",
    "    logger.info(f\"Загружены кэшированные эмбеддинги TRAIN: {X_train.shape}\")\n",
    "else:\n",
    "    X_train = bert_mean_embeddings(\n",
    "        df_train,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        device,\n",
    "        BATCH_SIZE,\n",
    "        MAX_LENGTH,\n",
    "        desc=\"Train embeddings\"\n",
    "    )\n",
    "    y_train = df_train[\"toxic\"].values\n",
    "    np.save(train_emb_path, X_train)\n",
    "    np.save(train_labels_path, y_train)\n",
    "    logger.info(f\"Сгенерированы и сохранены эмбеддинги TRAIN: {X_train.shape}\")\n",
    "\n",
    "# ===== TEST =====\n",
    "if os.path.exists(test_emb_path) and os.path.exists(test_labels_path):\n",
    "    X_test = np.load(test_emb_path)\n",
    "    y_test = np.load(test_labels_path)\n",
    "    logger.info(f\"Загружены кэшированные эмбеддинги TEST: {X_test.shape}\")\n",
    "else:\n",
    "    X_test = bert_mean_embeddings(\n",
    "        df_test,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        device,\n",
    "        BATCH_SIZE,\n",
    "        MAX_LENGTH,\n",
    "        desc=\"Test embeddings\"\n",
    "    )\n",
    "    y_test = df_test[\"toxic\"].values\n",
    "    np.save(test_emb_path, X_test)\n",
    "    np.save(test_labels_path, y_test)\n",
    "    logger.info(f\"Сгенерированы и сохранены эмбеддинги TEST: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 20:20:49 [INFO] __main__: LogisticRegression: Trial 0, средний F1 = 0.6840\n",
      "2026-01-07 20:21:17 [INFO] __main__: LogisticRegression: Trial 1, средний F1 = 0.6834\n",
      "2026-01-07 20:21:19 [INFO] __main__: RidgeClassifier: Trial 0, средний F1 = 0.6529\n",
      "2026-01-07 20:21:20 [INFO] __main__: RidgeClassifier: Trial 1, средний F1 = 0.6529\n",
      "2026-01-07 20:22:55 [INFO] __main__: LGBM: Trial 0, средний F1 = 0.7412\n",
      "2026-01-07 20:23:48 [INFO] __main__: LGBM: Trial 1, средний F1 = 0.6423\n",
      "2026-01-07 20:34:38 [INFO] __main__: XGB: Trial 0, средний F1 = 0.7175\n",
      "2026-01-07 20:38:24 [INFO] __main__: XGB: Trial 1, средний F1 = 0.6928\n",
      "2026-01-07 20:39:34 [INFO] __main__: Лучшая модель: LGBM\n",
      "2026-01-07 20:39:34 [INFO] __main__: Параметры: {'n_estimators': 315, 'max_depth': 10, 'learning_rate': 0.1615956698742364}\n",
      "2026-01-07 20:39:34 [INFO] __main__: F1 на test: 0.7445\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "models_to_run = [\"LogisticRegression\", \"RidgeClassifier\", \"LGBM\", \"XGB\"]\n",
    "\n",
    "for model_name in models_to_run:\n",
    "\n",
    "    def objective(trial):\n",
    "        # --- выбираем модель и параметры ---\n",
    "        if model_name == \"LogisticRegression\":\n",
    "            C = trial.suggest_float(\"C\", 1e-3, 10.0, log=True)\n",
    "            clf = LogisticRegression(C=C, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "        elif model_name == \"RidgeClassifier\":\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "            clf = RidgeClassifier(alpha=alpha, class_weight=\"balanced\")\n",
    "        elif model_name == \"LGBM\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "            clf = LGBMClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                class_weight=\"balanced\",\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "        elif model_name == \"XGB\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "            clf = XGBClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric=\"logloss\",\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )\n",
    "\n",
    "        # --- кросс-валидация с прунингом ---\n",
    "        cv = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RANDOM_STATE)\n",
    "        f1_scores = []\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_pred = clf.predict(X_val)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            # прунинг: если среднее по текущим фолдам хуже медианы, отбрасываем trial\n",
    "            intermediate_value = np.mean(f1_scores)\n",
    "            trial.report(intermediate_value, step=fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        logger.info(f\"{model_name}: Trial {trial.number}, средний F1 = {mean_f1:.4f}\")\n",
    "        return mean_f1\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1),\n",
    "        sampler=TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=N_OPTUNA)\n",
    "\n",
    "    best_models[model_name] = {\n",
    "        \"best_trial\": study.best_trial,\n",
    "        \"best_value\": study.best_value\n",
    "    }\n",
    "\n",
    "# --- выбираем лучшую модель среди всех ---\n",
    "best_model_name = max(best_models, key=lambda k: best_models[k][\"best_value\"])\n",
    "best_trial = best_models[best_model_name][\"best_trial\"]\n",
    "params = best_trial.params\n",
    "\n",
    "# --- финальная модель ---\n",
    "if best_model_name == \"LogisticRegression\":\n",
    "    final_model = LogisticRegression(**params, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "elif best_model_name == \"RidgeClassifier\":\n",
    "    final_model = RidgeClassifier(**params, class_weight=\"balanced\")\n",
    "elif best_model_name == \"LGBM\":\n",
    "    final_model = LGBMClassifier(**params, class_weight=\"balanced\", n_jobs=-1, verbose=-1)\n",
    "elif best_model_name == \"XGB\":\n",
    "    final_model = XGBClassifier(**params, use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1, verbosity=0)\n",
    "\n",
    "# обучение на всем трейне\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# оценка на тесте\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "logger.info(f\"Лучшая модель: {best_model_name}\")\n",
    "logger.info(f\"Параметры: {params}\")\n",
    "logger.info(f\"F1 на test: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нейросеть\n",
    "# --- данные для CV (ТОЛЬКО train) ---\n",
    "X_cv = X_train.astype(np.float32)\n",
    "y_cv = y_train.astype(np.int64)\n",
    "\n",
    "# --- простая MLP ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim1, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- обучение одной модели с ранней остановкой ---\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    lr,\n",
    "    batch_size,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    patience=EARLY_STOP\n",
    "):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ВАЖНО: веса считаем ТОЛЬКО по train части\n",
    "    y_train_local = train_dataset.tensors[1].numpy()\n",
    "    class_counts = np.bincount(y_train_local)\n",
    "    class_weights = class_counts.sum() / (2 * class_counts)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    trigger = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # --- validation ---\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(yb.cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = model.state_dict()\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_val_f1\n",
    "\n",
    "# --- Optuna objective с CV (БЕЗ УТЕЧЕК) ---\n",
    "def objective_mlp(trial):\n",
    "    hidden_dim1 = trial.suggest_int(\"hidden_dim1\", 512, 1024)\n",
    "    hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 256, 512)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.15)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_cv, y_cv):\n",
    "        train_ds = TensorDataset(\n",
    "            torch.from_numpy(X_cv[train_idx]),\n",
    "            torch.from_numpy(y_cv[train_idx])\n",
    "        )\n",
    "        val_ds = TensorDataset(\n",
    "            torch.from_numpy(X_cv[val_idx]),\n",
    "            torch.from_numpy(y_cv[val_idx])\n",
    "        )\n",
    "\n",
    "        model = MLP(\n",
    "            input_dim=X_cv.shape[1],\n",
    "            hidden_dim1=hidden_dim1,\n",
    "            hidden_dim2=hidden_dim2,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        _, val_f1 = train_model(\n",
    "            model,\n",
    "            train_ds,\n",
    "            val_ds,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        f1_scores.append(val_f1)\n",
    "\n",
    "    return float(np.mean(f1_scores))\n",
    "\n",
    "# --- запуск Optuna ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_mlp, n_trials=N_OPTUNA)\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "logger.info(f\"Лучшие параметры: {best_params}\")\n",
    "logger.info(f\"F1 CV (train): {study.best_value:.4f}\")\n",
    "\n",
    "# --- финальная модель: обучение НА ВСЁМ train ---\n",
    "final_model = MLP(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim1=best_params[\"hidden_dim1\"],\n",
    "    hidden_dim2=best_params[\"hidden_dim2\"],\n",
    "    dropout=best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "full_train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train.astype(np.float32)),\n",
    "    torch.from_numpy(y_train.astype(np.int64))\n",
    ")\n",
    "\n",
    "# используем train как val только для early stopping\n",
    "final_model, _ = train_model(\n",
    "    final_model,\n",
    "    full_train_dataset,\n",
    "    full_train_dataset,\n",
    "    lr=best_params[\"lr\"],\n",
    "    batch_size=best_params[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# --- оценка на test (holdout) ---\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test.astype(np.float32)),\n",
    "    torch.from_numpy(y_test.astype(np.int64))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "final_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = final_model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(yb.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "test_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "logger.info(f\"F1 test: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
