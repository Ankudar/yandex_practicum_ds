{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymystem3 -q\n",
    "# !pip install transformers -q\n",
    "# !pip install emoji -q\n",
    "# !pip install imblearn -q\n",
    "# !pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "import logging\n",
    "import warnings\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Научные и аналитические библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Библиотеки для бустинга\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Работа с текстом\n",
    "import emoji\n",
    "\n",
    "# PyTorch и Transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Оптимизация гиперпараметров\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Прочие полезные инструменты\n",
    "from tqdm import tqdm, notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Отключение предупреждений\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 22:43:19 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-07 22:43:22 [WARNING] huggingface_hub.file_download: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-07 22:43:34 [INFO] __main__: SentenceTransformer on cuda\n"
     ]
    }
   ],
   "source": [
    "# # --- BERT ---\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "# # device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # torch.compile — ТОЛЬКО если поддерживается\n",
    "# if device.type == \"cuda\":\n",
    "#     try:\n",
    "#         model = torch.compile(model)\n",
    "#         logger.info(\"torch.compile активен\")\n",
    "#     except RuntimeError as e:\n",
    "#         logger.warning(f\"torch.compile отключен: {e}\")\n",
    "\n",
    "# logger.info(device)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "logger.info(f\"SentenceTransformer on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "CV = 3\n",
    "N_OPTUNA = 2\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 20\n",
    "N_EPOCHS = 10000\n",
    "EARLY_STOP = 5\n",
    "\n",
    "def seed_everything(seed=RANDOM_STATE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "650822e4-5fc0-47f7-b32f-ab28c7f126ca",
       "rows": [
        [
         "0",
         "Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27",
         "0"
        ],
        [
         "1",
         "D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)",
         "0"
        ],
        [
         "2",
         "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.",
         "0"
        ],
        [
         "3",
         "\"\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"",
         "0"
        ],
        [
         "4",
         "You, sir, are my hero. Any chance you remember what page that's on?",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', index_col=[0])\n",
    "df = pd.read_csv('./data/toxic_comments.csv', index_col=[0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:02<00:00, 54486.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def bert_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Очистка текста для BERT:\n",
    "    - убираем ссылки,\n",
    "    - заменяем несколько пробелов на один,\n",
    "    - убираем эмодзи,\n",
    "    - приводим к нижнему регистру,\n",
    "    - обрезаем лишние пробелы по краям.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# применяем к датафрейму\n",
    "df['text_clean'] = df['text'].progress_apply(bert_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_clean",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d6e8b1e9-1788-4004-a35c-0a7f13c64e1c",
       "rows": [
        [
         "74297",
         "Adoption?\nHi, Katie. I am also interested in adopting you. I have been a member for a long time, and edit many pages in German, French, and English. I am 3rd generation American, decending from Austrian/German heritage. Are you still interested in adoption? I would love to help you.",
         "0",
         "adoption? hi, katie. i am also interested in adopting you. i have been a member for a long time, and edit many pages in german, french, and english. i am 3rd generation american, decending from austrian/german heritage. are you still interested in adoption? i would love to help you."
        ],
        [
         "53437",
         "\"\n\n WP:APPLE's Backlog Elimination Drive is over! \n\nAs the coordinator of the WikiProject Apple Inc. Backlog Elimination Drive for May 2010, I would like to thank you for your active participation in the drive. \n\nI am writing to inform you that we have assessed all project articles.  We thank all of our participants, as we couldn't have done it without you!\n\nOnce again, thank you for participating, and we appreciate the meaningful drop in the numbers due to your hard work and efforts.  Awards will be delivered soon to our participants!\n\nCheers, mono \"",
         "0",
         "\" wp:apple's backlog elimination drive is over! as the coordinator of the wikiproject apple inc. backlog elimination drive for may 2010, i would like to thank you for your active participation in the drive. i am writing to inform you that we have assessed all project articles. we thank all of our participants, as we couldn't have done it without you! once again, thank you for participating, and we appreciate the meaningful drop in the numbers due to your hard work and efforts. awards will be delivered soon to our participants! cheers, mono \""
        ],
        [
         "68821",
         "Your recent removal of some text at Global Positioning System\n\nWas hilarious. Thank you. I snorted tea out my nose. ♠♠ (talk)",
         "0",
         "your recent removal of some text at global positioning system was hilarious. thank you. i snorted tea out my nose. ♠♠ (talk)"
        ],
        [
         "14003",
         "No, we don't need anyone to say explicitly that the dubious genera don't exist; we just need a comprehensive listing of the genera that are present. At the moment, the only one of those post-dating the original description is WoRMS. Once it fixes its error, everything will be fine, but we can't lead the way. We can choose as editors to disregard WoRMS, which would involve removing the whole last paragraph. No-one has argued for that yet, but I could see it working. I'll give it a go.",
         "0",
         "no, we don't need anyone to say explicitly that the dubious genera don't exist; we just need a comprehensive listing of the genera that are present. at the moment, the only one of those post-dating the original description is worms. once it fixes its error, everything will be fine, but we can't lead the way. we can choose as editors to disregard worms, which would involve removing the whole last paragraph. no-one has argued for that yet, but i could see it working. i'll give it a go."
        ],
        [
         "155348",
         "Piss off, she is an ignorant bitch.",
         "1",
         "piss off, she is an ignorant bitch."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74297</th>\n",
       "      <td>Adoption?\\nHi, Katie. I am also interested in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>adoption? hi, katie. i am also interested in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53437</th>\n",
       "      <td>\"\\n\\n WP:APPLE's Backlog Elimination Drive is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\" wp:apple's backlog elimination drive is over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68821</th>\n",
       "      <td>Your recent removal of some text at Global Pos...</td>\n",
       "      <td>0</td>\n",
       "      <td>your recent removal of some text at global pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003</th>\n",
       "      <td>No, we don't need anyone to say explicitly tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, we don't need anyone to say explicitly tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155348</th>\n",
       "      <td>Piss off, she is an ignorant bitch.</td>\n",
       "      <td>1</td>\n",
       "      <td>piss off, she is an ignorant bitch.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "74297   Adoption?\\nHi, Katie. I am also interested in ...      0   \n",
       "53437   \"\\n\\n WP:APPLE's Backlog Elimination Drive is ...      0   \n",
       "68821   Your recent removal of some text at Global Pos...      0   \n",
       "14003   No, we don't need anyone to say explicitly tha...      0   \n",
       "155348                Piss off, she is an ignorant bitch.      1   \n",
       "\n",
       "                                               text_clean  \n",
       "74297   adoption? hi, katie. i am also interested in a...  \n",
       "53437   \" wp:apple's backlog elimination drive is over...  \n",
       "68821   your recent removal of some text at global pos...  \n",
       "14003   no, we don't need anyone to say explicitly tha...  \n",
       "155348                piss off, she is an ignorant bitch.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 22:43:38 [INFO] __main__: Train size: 127433, Test size: 31859\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"toxic\"]\n",
    ")\n",
    "\n",
    "# сброс индексов\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_mean_embeddings(df_part, tokenizer, model, device, batch_size, max_length, desc):\n",
    "#     all_embeddings = []\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     for i in tqdm(range(0, len(df_part), batch_size), desc=desc):\n",
    "#         batch_texts = df_part['text_clean'].iloc[i:i + batch_size].tolist()\n",
    "\n",
    "#         encoded = tokenizer(\n",
    "#             batch_texts,\n",
    "#             padding=True,          # без padding до max_length\n",
    "#             truncation=True,\n",
    "#             max_length=max_length,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         input_ids = encoded[\"input_ids\"].to(device)\n",
    "#         attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 return_dict=True\n",
    "#             )\n",
    "\n",
    "#             # CLS embedding\n",
    "#             embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "#             # L2-нормализация (важно для линейных моделей)\n",
    "#             embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "#         all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "#     return np.vstack(all_embeddings)\n",
    "\n",
    "def get_sentence_embeddings(df_part, model, batch_size, desc):\n",
    "    return model.encode(\n",
    "        df_part[\"text_clean\"].tolist(),\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1992/1992 [00:39<00:00, 50.56it/s] \n",
      "2026-01-07 22:44:21 [INFO] __main__: Сгенерированы и сохранены эмбеддинги TRAIN: (127433, 384)\n",
      "Batches: 100%|██████████| 498/498 [00:09<00:00, 49.87it/s] \n",
      "2026-01-07 22:44:31 [INFO] __main__: Сгенерированы и сохранены эмбеддинги TEST: (31859, 384)\n"
     ]
    }
   ],
   "source": [
    "# пути для сохранения эмбеддингов\n",
    "train_emb_path = \"./data/X_train_emb.npy\"\n",
    "test_emb_path = \"./data/X_test_emb.npy\"\n",
    "train_labels_path = \"./data/y_train.npy\"\n",
    "test_labels_path = \"./data/y_test.npy\"\n",
    "\n",
    "# ===== TRAIN =====\n",
    "if os.path.exists(train_emb_path) and os.path.exists(train_labels_path):\n",
    "    X_train = np.load(train_emb_path)\n",
    "    y_train = np.load(train_labels_path)\n",
    "    logger.info(f\"Загружены кэшированные эмбеддинги TRAIN: {X_train.shape}\")\n",
    "else:\n",
    "    X_train = get_sentence_embeddings(\n",
    "        df_train,\n",
    "        model,\n",
    "        BATCH_SIZE,\n",
    "        desc=\"Train embeddings\"\n",
    "    )\n",
    "    y_train = df_train[\"toxic\"].values\n",
    "    np.save(train_emb_path, X_train)\n",
    "    np.save(train_labels_path, y_train)\n",
    "    logger.info(f\"Сгенерированы и сохранены эмбеддинги TRAIN: {X_train.shape}\")\n",
    "\n",
    "# ===== TEST =====\n",
    "if os.path.exists(test_emb_path) and os.path.exists(test_labels_path):\n",
    "    X_test = np.load(test_emb_path)\n",
    "    y_test = np.load(test_labels_path)\n",
    "    logger.info(f\"Загружены кэшированные эмбеддинги TEST: {X_test.shape}\")\n",
    "else:\n",
    "    X_test = get_sentence_embeddings(\n",
    "        df_test,\n",
    "        model,\n",
    "        BATCH_SIZE,\n",
    "        desc=\"Test embeddings\"\n",
    "    )\n",
    "    y_test = df_test[\"toxic\"].values\n",
    "    np.save(test_emb_path, X_test)\n",
    "    np.save(test_labels_path, y_test)\n",
    "    logger.info(f\"Сгенерированы и сохранены эмбеддинги TEST: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 22:44:31 [INFO] __main__: Распределение классов в train: [114484  12949]\n",
      "2026-01-07 22:44:31 [INFO] __main__: Доля positive класса: 0.102\n"
     ]
    }
   ],
   "source": [
    "# Проверяем баланс классов\n",
    "logger.info(f\"Распределение классов в train: {np.bincount(y_train)}\")\n",
    "logger.info(f\"Доля positive класса: {y_train.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 22:53:44 [INFO] __main__: ===== START MODEL: LogisticRegression =====\n",
      "2026-01-07 22:53:52 [INFO] __main__: [LogisticRegression] Trial 0 | Mean F1=0.7175\n",
      "2026-01-07 22:53:59 [INFO] __main__: [LogisticRegression] Trial 1 | Mean F1=0.7283\n",
      "2026-01-07 22:54:08 [INFO] __main__: [LogisticRegression] Trial 2 | Mean F1=0.7274\n",
      "2026-01-07 22:54:15 [INFO] __main__: [LogisticRegression] Trial 3 | Mean F1=0.7263\n",
      "2026-01-07 22:54:24 [INFO] __main__: [LogisticRegression] Trial 4 PRUNED | Mean F1=0.6385\n",
      "2026-01-07 22:54:29 [INFO] __main__: [LogisticRegression] Trial 5 PRUNED | Mean F1=0.7219\n",
      "2026-01-07 22:54:33 [INFO] __main__: [LogisticRegression] Trial 6 PRUNED | Mean F1=0.6952\n",
      "2026-01-07 22:54:37 [INFO] __main__: [LogisticRegression] Trial 7 PRUNED | Mean F1=0.7109\n",
      "2026-01-07 22:54:42 [INFO] __main__: [LogisticRegression] Trial 8 PRUNED | Mean F1=0.7195\n",
      "2026-01-07 22:54:46 [INFO] __main__: [LogisticRegression] Trial 9 PRUNED | Mean F1=0.6652\n",
      "2026-01-07 22:54:46 [INFO] __main__: ===== END MODEL: LogisticRegression | Best F1=0.7283 =====\n",
      "2026-01-07 22:54:46 [INFO] __main__: ===== START MODEL: RidgeClassifier =====\n",
      "2026-01-07 22:54:47 [INFO] __main__: [RidgeClassifier] Trial 0 | Mean F1=0.6295\n",
      "2026-01-07 22:54:48 [INFO] __main__: [RidgeClassifier] Trial 1 | Mean F1=0.6291\n",
      "2026-01-07 22:54:49 [INFO] __main__: [RidgeClassifier] Trial 2 PRUNED | Mean F1=0.6259\n",
      "2026-01-07 22:54:50 [INFO] __main__: [RidgeClassifier] Trial 3 | Mean F1=0.6294\n",
      "2026-01-07 22:54:51 [INFO] __main__: [RidgeClassifier] Trial 4 PRUNED | Mean F1=0.6267\n",
      "2026-01-07 22:54:52 [INFO] __main__: [RidgeClassifier] Trial 5 PRUNED | Mean F1=0.6265\n",
      "2026-01-07 22:54:52 [INFO] __main__: [RidgeClassifier] Trial 6 PRUNED | Mean F1=0.6267\n",
      "2026-01-07 22:54:53 [INFO] __main__: [RidgeClassifier] Trial 7 PRUNED | Mean F1=0.6266\n",
      "2026-01-07 22:54:54 [INFO] __main__: [RidgeClassifier] Trial 8 PRUNED | Mean F1=0.6266\n",
      "2026-01-07 22:54:55 [INFO] __main__: [RidgeClassifier] Trial 9 PRUNED | Mean F1=0.6267\n",
      "2026-01-07 22:54:55 [INFO] __main__: ===== END MODEL: RidgeClassifier | Best F1=0.6295 =====\n",
      "2026-01-07 22:54:55 [INFO] __main__: ===== START MODEL: LGBM =====\n",
      "2026-01-07 22:59:20 [INFO] __main__: [LGBM] Trial 0 | Mean F1=0.7234\n",
      "2026-01-07 23:02:03 [INFO] __main__: [LGBM] Trial 1 | Mean F1=0.7132\n",
      "[W 2026-01-07 23:04:51,273] Trial 2 failed with parameters: {'n_estimators': 492, 'learning_rate': 0.12773260386751642, 'num_leaves': 205, 'min_child_samples': 6, 'subsample': 0.7350081205398685, 'colsample_bytree': 0.9253842098463103} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\DM\\AppData\\Local\\Temp\\ipykernel_23116\\2284257804.py\", line 53, in objective\n",
      "    clf.fit(X_tr, y_tr)\n",
      "    ~~~~~~~^^^^^^^^^^^^\n",
      "  File \"e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1560, in fit\n",
      "    super().fit(\n",
      "    ~~~~~~~~~~~^\n",
      "        X,\n",
      "        ^^\n",
      "    ...<12 lines>...\n",
      "        init_model=init_model,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1049, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params=params,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        callbacks=callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\engine.py\", line 322, in train\n",
      "    booster.update(fobj=fobj)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\basic.py\", line 4155, in update\n",
      "    _LIB.LGBM_BoosterUpdateOneIter(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._handle,\n",
      "        ^^^^^^^^^^^^^\n",
      "        ctypes.byref(is_finished),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2026-01-07 23:04:51,293] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_f1\n\u001b[32m     82\u001b[39m study = optuna.create_study(\n\u001b[32m     83\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     84\u001b[39m     sampler=TPESampler(seed=RANDOM_STATE),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     )\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_OPTUNA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m logger.info(\n\u001b[32m     95\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m===== END MODEL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m =====\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m )\n\u001b[32m     99\u001b[39m best_models[model_name] = {\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_trial\u001b[39m\u001b[33m\"\u001b[39m: study.best_trial,\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_value\u001b[39m\u001b[33m\"\u001b[39m: study.best_value\n\u001b[32m    102\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     50\u001b[39m X_tr, X_val = X_train[train_idx], X_train[val_idx]\n\u001b[32m     51\u001b[39m y_tr, y_val = y_train[train_idx], y_train[val_idx]\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(clf, \u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     56\u001b[39m     y_val_proba = clf.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[39m, in \u001b[36mLGBMClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m             valid_sets.append((valid_x, \u001b[38;5;28mself\u001b[39m._le.transform(valid_y)))\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "\n",
    "models_to_run = [\"LogisticRegression\", \"RidgeClassifier\", \"LGBM\"]\n",
    "\n",
    "for model_name in models_to_run:\n",
    "\n",
    "    logger.info(f\"===== START MODEL: {model_name} =====\")\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        # ===== модель и гиперпараметры =====\n",
    "        if model_name == \"LogisticRegression\":\n",
    "            clf = LogisticRegression(\n",
    "                C=trial.suggest_float(\"C\", 1e-3, 10.0, log=True),\n",
    "                max_iter=2000,\n",
    "                class_weight=\"balanced\",\n",
    "                n_jobs=-1,\n",
    "                solver=\"lbfgs\"\n",
    "            )\n",
    "\n",
    "        elif model_name == \"RidgeClassifier\":\n",
    "            clf = RidgeClassifier(\n",
    "                alpha=trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "                class_weight=\"balanced\"\n",
    "            )\n",
    "\n",
    "        elif model_name == \"LGBM\":\n",
    "            clf = LGBMClassifier(\n",
    "                n_estimators=trial.suggest_int(\"n_estimators\", 100, 600),\n",
    "                learning_rate=trial.suggest_float(\"learning_rate\", 1e-2, 0.2, log=True),\n",
    "                num_leaves=trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "                min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "                subsample=trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "                colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
    "                class_weight=\"balanced\",\n",
    "                n_jobs=-1,\n",
    "                random_state=RANDOM_STATE,\n",
    "                verbose=-1\n",
    "            )\n",
    "\n",
    "        cv = StratifiedKFold(\n",
    "            n_splits=CV,\n",
    "            shuffle=True,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "        f1_scores = []\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            clf.fit(X_tr, y_tr)\n",
    "\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "                prec, rec, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "                f1 = np.max(2 * prec * rec / (prec + rec + 1e-9))\n",
    "            else:\n",
    "                f1 = f1_score(y_val, clf.predict(X_val))\n",
    "\n",
    "            f1_scores.append(float(f1))\n",
    "\n",
    "            # ===== PRUNER (ОБЯЗАТЕЛЬНО ВНУТРИ ФОЛДОВ) =====\n",
    "            trial.report(np.mean(f1_scores), step=fold_idx)\n",
    "            if trial.should_prune():\n",
    "                logger.info(\n",
    "                    f\"[{model_name}] Trial {trial.number} PRUNED | \"\n",
    "                    f\"Mean F1={np.mean(f1_scores):.4f}\"\n",
    "                )\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        mean_f1 = float(np.mean(f1_scores))\n",
    "\n",
    "        logger.info(\n",
    "            f\"[{model_name}] Trial {trial.number} | \"\n",
    "            f\"Mean F1={mean_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        return mean_f1\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=TPESampler(seed=RANDOM_STATE),\n",
    "        pruner=MedianPruner(\n",
    "            n_startup_trials=2,\n",
    "            n_warmup_steps=1,\n",
    "            interval_steps=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=N_OPTUNA)\n",
    "\n",
    "    logger.info(\n",
    "        f\"===== END MODEL: {model_name} | \"\n",
    "        f\"Best F1={study.best_value:.4f} =====\"\n",
    "    )\n",
    "\n",
    "    best_models[model_name] = {\n",
    "        \"best_trial\": study.best_trial,\n",
    "        \"best_value\": study.best_value\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # нейросеть\n",
    "# # --- данные для CV (ТОЛЬКО train) ---\n",
    "# X_cv = X_train.astype(np.float32)\n",
    "# y_cv = y_train.astype(np.int64)\n",
    "\n",
    "# # --- простая MLP ---\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim1, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # --- обучение одной модели с ранней остановкой ---\n",
    "# def train_model(\n",
    "#     model,\n",
    "#     train_dataset,\n",
    "#     val_dataset,\n",
    "#     lr,\n",
    "#     batch_size,\n",
    "#     n_epochs=N_EPOCHS,\n",
    "#     patience=EARLY_STOP\n",
    "# ):\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # ВАЖНО: веса считаем ТОЛЬКО по train части\n",
    "#     y_train_local = train_dataset.tensors[1].numpy()\n",
    "#     class_counts = np.bincount(y_train_local)\n",
    "#     class_weights = class_counts.sum() / (2 * class_counts)\n",
    "#     class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     best_val_f1 = 0.0\n",
    "#     best_state = None\n",
    "#     trigger = 0\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         # --- train ---\n",
    "#         model.train()\n",
    "#         for xb, yb in train_loader:\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # --- validation ---\n",
    "#         model.eval()\n",
    "#         all_preds, all_labels = [], []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for xb, yb in val_loader:\n",
    "#                 xb, yb = xb.to(device), yb.to(device)\n",
    "#                 logits = model(xb)\n",
    "#                 preds = torch.argmax(logits, dim=1)\n",
    "#                 all_preds.append(preds.cpu())\n",
    "#                 all_labels.append(yb.cpu())\n",
    "\n",
    "#         all_preds = torch.cat(all_preds)\n",
    "#         all_labels = torch.cat(all_labels)\n",
    "#         val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "#         if val_f1 > best_val_f1:\n",
    "#             best_val_f1 = val_f1\n",
    "#             best_state = model.state_dict()\n",
    "#             trigger = 0\n",
    "#         else:\n",
    "#             trigger += 1\n",
    "#             if trigger >= patience:\n",
    "#                 break\n",
    "\n",
    "#     model.load_state_dict(best_state)\n",
    "#     return model, best_val_f1\n",
    "\n",
    "# # --- Optuna objective с CV (БЕЗ УТЕЧЕК) ---\n",
    "# def objective_mlp(trial):\n",
    "#     hidden_dim1 = trial.suggest_int(\"hidden_dim1\", 512, 1024)\n",
    "#     hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 256, 512)\n",
    "#     dropout = trial.suggest_float(\"dropout\", 0.05, 0.15)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n",
    "#     batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "#     cv = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RANDOM_STATE)\n",
    "#     f1_scores = []\n",
    "\n",
    "#     for train_idx, val_idx in cv.split(X_cv, y_cv):\n",
    "#         train_ds = TensorDataset(\n",
    "#             torch.from_numpy(X_cv[train_idx]),\n",
    "#             torch.from_numpy(y_cv[train_idx])\n",
    "#         )\n",
    "#         val_ds = TensorDataset(\n",
    "#             torch.from_numpy(X_cv[val_idx]),\n",
    "#             torch.from_numpy(y_cv[val_idx])\n",
    "#         )\n",
    "\n",
    "#         model = MLP(\n",
    "#             input_dim=X_cv.shape[1],\n",
    "#             hidden_dim1=hidden_dim1,\n",
    "#             hidden_dim2=hidden_dim2,\n",
    "#             dropout=dropout\n",
    "#         )\n",
    "\n",
    "#         _, val_f1 = train_model(\n",
    "#             model,\n",
    "#             train_ds,\n",
    "#             val_ds,\n",
    "#             lr=lr,\n",
    "#             batch_size=batch_size\n",
    "#         )\n",
    "\n",
    "#         f1_scores.append(val_f1)\n",
    "\n",
    "#     return float(np.mean(f1_scores))\n",
    "\n",
    "# # --- запуск Optuna ---\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective_mlp, n_trials=N_OPTUNA)\n",
    "\n",
    "# best_params = study.best_trial.params\n",
    "# logger.info(f\"Лучшие параметры: {best_params}\")\n",
    "# logger.info(f\"F1 CV (train): {study.best_value:.4f}\")\n",
    "\n",
    "# # --- финальная модель: обучение НА ВСЁМ train ---\n",
    "# final_model = MLP(\n",
    "#     input_dim=X_train.shape[1],\n",
    "#     hidden_dim1=best_params[\"hidden_dim1\"],\n",
    "#     hidden_dim2=best_params[\"hidden_dim2\"],\n",
    "#     dropout=best_params[\"dropout\"]\n",
    "# )\n",
    "\n",
    "# full_train_dataset = TensorDataset(\n",
    "#     torch.from_numpy(X_train.astype(np.float32)),\n",
    "#     torch.from_numpy(y_train.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# # используем train как val только для early stopping\n",
    "# final_model, _ = train_model(\n",
    "#     final_model,\n",
    "#     full_train_dataset,\n",
    "#     full_train_dataset,\n",
    "#     lr=best_params[\"lr\"],\n",
    "#     batch_size=best_params[\"batch_size\"]\n",
    "# )\n",
    "\n",
    "# # --- оценка на test (holdout) ---\n",
    "# test_dataset = TensorDataset(\n",
    "#     torch.from_numpy(X_test.astype(np.float32)),\n",
    "#     torch.from_numpy(y_test.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=best_params[\"batch_size\"],\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# final_model.eval()\n",
    "# all_preds, all_labels = [], []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for xb, yb in test_loader:\n",
    "#         xb, yb = xb.to(device), yb.to(device)\n",
    "#         logits = final_model(xb)\n",
    "#         preds = torch.argmax(logits, dim=1)\n",
    "#         all_preds.append(preds.cpu())\n",
    "#         all_labels.append(yb.cpu())\n",
    "\n",
    "# all_preds = torch.cat(all_preds)\n",
    "# all_labels = torch.cat(all_labels)\n",
    "\n",
    "# test_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "# logger.info(f\"F1 test: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
