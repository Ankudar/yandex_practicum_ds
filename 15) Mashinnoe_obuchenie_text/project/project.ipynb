{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymystem3 -q\n",
    "# !pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\my_github\\yandex_practicum_ds\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import transformers\n",
    "import nltk\n",
    "import logging\n",
    "import warnings\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "# Оптимизация гиперпараметров\n",
    "import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Прочее\n",
    "from tqdm import tqdm, notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Отключение предупреждений\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгрузки\n",
    "# bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "# stopwords\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = nltk_stopwords.words('english')\n",
    "\n",
    "# cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # обязательно для инференса\n",
    "logger.info(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512 # макс длина для bert\n",
    "BATCH_SIZE = 100\n",
    "CV = 5\n",
    "N_OPTUNA = 5\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', index_col=[0])\n",
    "df = pd.read_csv('./data/toxic_comments.csv', index_col=[0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "corpus = df['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# почистим текст\n",
    "def fast_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|@\\w+|#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in english_stopwords]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['text_clean'] = df['text'].progress_apply(fast_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приведем к лемам\n",
    "# попробовал в рамках тест - более суток обработка, ну ее... пусть пока лежит закомменченное\n",
    "# либо я что-то не так делаю...\n",
    "# def lemmatize(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     return \" \".join(m.lemmatize(text))\n",
    "\n",
    "# df['lema_text'] = df['text_clean'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 0 -> берет все данные\n",
    "N = 0\n",
    "\n",
    "# разбиваем по классам\n",
    "df_0 = df[df['toxic'] == 0]\n",
    "df_1 = df[df['toxic'] == 1]\n",
    "\n",
    "if N == 0:\n",
    "    df_0_sample = df_0\n",
    "    df_1_sample = df_1\n",
    "else:\n",
    "    n_samples = N // 2  # половина от N для каждого класса\n",
    "    df_0_sample = resample(df_0, n_samples=n_samples, replace=False, random_state=RANDOM_STATE)\n",
    "    df_1_sample = resample(df_1, n_samples=n_samples, replace=False, random_state=RANDOM_STATE)\n",
    "\n",
    "# объединяем и перемешиваем\n",
    "df_sample = pd.concat([df_0_sample, df_1_sample]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(df_sample), BATCH_SIZE), desc=\"Tokenizing & Embedding\"):\n",
    "    batch_texts = df_sample['text_clean'].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    \n",
    "    # батчевая токенизация\n",
    "    encoded = tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # переносим на GPU\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "    # инференс через BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # CLS-токен, переводим в numpy (CPU)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    all_embeddings.append(cls_embeddings)\n",
    "\n",
    "# объединяем все батчи в один массив\n",
    "features = np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = df_sample['toxic']\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"LogisticRegression\", \"DecisionTree\", \"RandomForest\", \"LGBM\"])\n",
    "\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        C = trial.suggest_float(\"C\", 1e-3, 10.0, log=True)\n",
    "        clf = LogisticRegression(C=C, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "    \n",
    "    elif model_name == \"DecisionTree\":\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, class_weight=\"balanced\")\n",
    "    \n",
    "    elif model_name == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    else:  # LGBM\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "    # Кросс-валидация на трейне\n",
    "    cv = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=\"f1\")\n",
    "    \n",
    "    # компактный вывод F1 для этой модели\n",
    "    logger.info(f\"{model_name}: средний F1 на трейне CV = {scores.mean():.4f}\")\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Запуск Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_OPTUNA)  # можно увеличить\n",
    "\n",
    "# финальная лучшая модель\n",
    "best_trial = study.best_trial\n",
    "best_model_name = best_trial.params.pop(\"model\")\n",
    "\n",
    "if best_model_name == \"LogisticRegression\":\n",
    "    final_model = LogisticRegression(**best_trial.params, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "elif best_model_name == \"DecisionTree\":\n",
    "    final_model = DecisionTreeClassifier(**best_trial.params, class_weight=\"balanced\")\n",
    "elif best_model_name == \"RandomForest\":\n",
    "    final_model = RandomForestClassifier(**best_trial.params, class_weight=\"balanced\", n_jobs=-1)\n",
    "else:\n",
    "    final_model = LGBMClassifier(**best_trial.params, class_weight=\"balanced\", n_jobs=-1, verbose=-1)\n",
    "\n",
    "# обучение на всём трейне\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# оценка на тесте\n",
    "y_pred = final_model.predict(X_val)\n",
    "test_f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "logger.info(f\"Лучшая модель: {best_model_name}\")\n",
    "logger.info(f\"Параметры: {best_trial.params}\")\n",
    "logger.info(f\"F1 на тесте: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
