{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9669be",
   "metadata": {},
   "source": [
    "# Проект Маркетинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58054e0e",
   "metadata": {},
   "source": [
    "## Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d18144",
   "metadata": {},
   "source": [
    "Интернет-магазин собирает историю покупателей, проводит рассылки предложений и планирует будущие продажи. Для оптимизации процессов надо выделить пользователей, которые готовы совершить покупку в ближайшее время."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e738a",
   "metadata": {},
   "source": [
    "### Цель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef6ec5",
   "metadata": {},
   "source": [
    "Предсказать вероятность покупки в течение 90 дней"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c818b8",
   "metadata": {},
   "source": [
    "### Задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f2539",
   "metadata": {},
   "source": [
    "● Изучить данные  \n",
    "● Разработать полезные признаки  \n",
    "● Создать модель для классификации пользователей  \n",
    "● Улучшить модель и максимизировать метрику roc_auc  \n",
    "● Выполнить тестирование  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46d6d6",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762824c7",
   "metadata": {},
   "source": [
    "apparel-purchases  \n",
    "история покупок  \n",
    "● client_id идентификатор пользователя  \n",
    "● quantity количество товаров в заказе  \n",
    "● price цена товара  \n",
    "● category_ids вложенные категории, к которым отнсится товар  \n",
    "● date дата покупки  \n",
    "● message_id идентификатор сообщения из рассылки  \n",
    "\n",
    "apparel-messages  \n",
    "история рекламных рассылок  \n",
    "● bulk_campaign_id идентификатор рекламной кампании  \n",
    "● client_id идентификатор пользователя  \n",
    "● message_id идентификатор сообщений  \n",
    "● event тип действия  \n",
    "● channel канал рассылки  \n",
    "● date дата рассылки  \n",
    "● created_at точное время создания сообщения  \n",
    "\n",
    "apparel-target_binary  \n",
    "совершит ли клиент покупку в течение следующих 90 дней  \n",
    "● client_id идентификатор пользователя  \n",
    "● target целевой признак  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad7c92",
   "metadata": {},
   "source": [
    "## Подготовка к работе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db99de",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5200ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Стандартная библиотека ===\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# === Научный стек ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Визуализация пропусков ===\n",
    "import missingno as msno\n",
    "\n",
    "# === IPython / Jupyter ===\n",
    "from IPython.display import HTML, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Статистика ===\n",
    "from phik import phik_matrix\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# === Sklearn ===\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "# === ML-библиотеки ===\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping as lgb_early_stopping, log_evaluation as lgb_log_evaluation\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# === Optuna ===\n",
    "import optuna\n",
    "from optuna.integration.sklearn import OptunaSearchCV\n",
    "import optuna.integration.lightgbm as lgb_opt\n",
    "import optuna.integration.catboost as cat_opt\n",
    "from optuna.integration import XGBoostPruningCallback, CatBoostPruningCallback, LightGBMPruningCallback\n",
    "from optuna import Study\n",
    "\n",
    "\n",
    "# === Настройки ===\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"sklearn.feature_selection._univariate_selection\")\n",
    "logging.getLogger(\"sklearn\").setLevel(logging.ERROR)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "xgb_params = {\"verbosity\": 0}\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec596867",
   "metadata": {},
   "source": [
    "### Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073573",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 20\n",
    "TEST_SIZE = 0.25\n",
    "N_JOBS = -1\n",
    "N_ITER = 10000 # число итераций для перебора и поиска лучших параметров\n",
    "N_CROSS_VALL = 3\n",
    "EARLY_STOP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb62051",
   "metadata": {},
   "source": [
    "### Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# форматирования текста\n",
    "def format_display(text):\n",
    "    return HTML(f\"<span style='font-size: 1.5em; font-weight: bold; font-style: italic;'>{text}</span>\")\n",
    "\n",
    "# сделаем функцию оценки пропусков в датасетах\n",
    "def missing_data(data):\n",
    "    missing_data = data.isna().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    display(missing_data)\n",
    "\n",
    "# функция для обработки пробелов\n",
    "def process_spaces(s):\n",
    "    if isinstance(s, str):\n",
    "        s = s.strip()\n",
    "        s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "# замена пробелов на нижнее подчеркинвание в названии столбцов\n",
    "def replace_spaces(s):\n",
    "    if isinstance(s, str):\n",
    "        s = s.strip()\n",
    "        s = '_'.join(s.split())\n",
    "    return s\n",
    "\n",
    "def drop_duplicated(data):\n",
    "    # проверка дубликатов\n",
    "    display(format_display(\"Проверим дубликаты и удалим, если есть\"))\n",
    "    num_duplicates = data.duplicated().sum()\n",
    "    display(num_duplicates)\n",
    "    \n",
    "    if num_duplicates > 0:\n",
    "        display(\"Удаляем\")\n",
    "        data = data.drop_duplicates(keep='first').reset_index(drop=True)  # обновляем DataFrame\n",
    "    else:\n",
    "        display(\"Дубликаты отсутствуют\")\n",
    "    return data\n",
    "\n",
    "def normalize_columns(columns):\n",
    "    new_cols = []\n",
    "    for col in columns:\n",
    "        # вставляем \"_\" перед заглавной буквой (латиница или кириллица), кроме первой\n",
    "        col = re.sub(r'(?<!^)(?=[A-ZА-ЯЁ])', '_', col)\n",
    "        # приводим к нижнему регистру\n",
    "        col = col.lower()\n",
    "        new_cols.append(col)\n",
    "    return new_cols\n",
    "\n",
    "def check_data(data):\n",
    "    # приведем все к нижнему регистру\n",
    "    data.columns = normalize_columns(data.columns)\n",
    "    \n",
    "    # удалим лишние пробелы в строках\n",
    "    data = data.map(process_spaces)\n",
    "\n",
    "    # и в названии столбцов\n",
    "    data.columns = [replace_spaces(col) for col in data.columns]\n",
    "    \n",
    "    # общая информация \n",
    "    display(format_display(\"Общая информация базы данных\"))\n",
    "    display(data.info())\n",
    "    \n",
    "    # 5 строк\n",
    "    display(format_display(\"5 случайных строк\"))\n",
    "    display(data.sample(5))\n",
    "    \n",
    "    # пропуски\n",
    "    display(format_display(\"Число пропусков в базе данных\"))\n",
    "    display(missing_data(data))\n",
    "\n",
    "    # проверка на наличие пропусков\n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        display(format_display(\"Визуализация пропусков\"))\n",
    "        msno.bar(data)\n",
    "        plt.show()\n",
    "        \n",
    "    # средние характеристики\n",
    "    display(format_display(\"Характеристики базы данных\"))\n",
    "    display(data.describe().T)\n",
    "    \n",
    "    # data = drop_duplicated(data)\n",
    "    \n",
    "    return data  # возвращаем измененные данные\n",
    "\n",
    "def parse_category_ids(x):\n",
    "    if isinstance(x, str):\n",
    "        return ast.literal_eval(x)\n",
    "    return x\n",
    "\n",
    "def plot_combined(data, col=None, target=None, col_type=None, legend_loc='best'):\n",
    "    \"\"\"\n",
    "    Строит графики для числовых столбцов в DataFrame, автоматически определяя их типы (дискретные или непрерывные).\n",
    "\n",
    "    :param data: DataFrame, содержащий данные для визуализации.\n",
    "    :param col: Список столбцов для построения графиков. Если None, будут использованы все числовые столбцы.\n",
    "    :param target: Столбец, по которому будет производиться разделение (для hue в графиках).\n",
    "    :param col_type: Словарь, определяющий типы столбцов ('col' для непрерывных и 'dis' для дискретных).\n",
    "                     Если None, типы будут определены автоматически.\n",
    "    :param legend_loc: Положение легенды для графиков (по умолчанию 'best').\n",
    "    :return: None. Графики отображаются с помощью plt.show().\n",
    "    \"\"\"\n",
    "    \n",
    "    # Определяем числовые столбцы\n",
    "    if col is None:\n",
    "        numerical_columns = data.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "    else:\n",
    "        numerical_columns = col\n",
    "\n",
    "    # Если col_type не указан, определяем типы автоматически\n",
    "    if col_type is None:\n",
    "        col_type = {}\n",
    "        for col in numerical_columns:\n",
    "            unique_count = data[col].nunique()\n",
    "            if unique_count > 20:\n",
    "                col_type[col] = 'col'  # Непрерывные данные\n",
    "            else:\n",
    "                col_type[col] = 'dis'  # Дискретные данные\n",
    "\n",
    "    total_plots = len(numerical_columns) * 2\n",
    "    ncols = 2\n",
    "    nrows = (total_plots + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 5 * nrows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        # Определяем тип графика\n",
    "        plot_type = col_type.get(col)\n",
    "        if plot_type is None:\n",
    "            raise ValueError(f\"Тип для столбца '{col}' не указан в col_type.\")\n",
    "\n",
    "        # Гистограмма или countplot\n",
    "        if index < len(axs):\n",
    "            if plot_type == 'col':\n",
    "                if target is not None:\n",
    "                    sns.histplot(data, x=col, hue=target, bins=20, kde=True, ax=axs[index])\n",
    "                    handles, labels = axs[index].get_legend_handles_labels()\n",
    "                    if handles:\n",
    "                        axs[index].legend(title=target, loc=legend_loc)\n",
    "                else:\n",
    "                    sns.histplot(data[col].dropna(), bins=20, kde=True, ax=axs[index])\n",
    "                axs[index].set_title(f'Гистограмма: {col}')\n",
    "            elif plot_type == 'dis':\n",
    "                if target is not None:\n",
    "                    sns.countplot(data=data, x=col, hue=target, ax=axs[index])\n",
    "                    handles, labels = axs[index].get_legend_handles_labels()\n",
    "                    if handles:\n",
    "                        axs[index].legend(title=target, loc=legend_loc)\n",
    "                else:\n",
    "                    sns.countplot(data=data, x=col, ax=axs[index])\n",
    "                axs[index].set_title(f'Countplot: {col}')\n",
    "                # поворот подписей X для дискретных\n",
    "                axs[index].tick_params(axis='x', rotation=90)\n",
    "            index += 1\n",
    "\n",
    "        # Боксплот\n",
    "        if index < len(axs):\n",
    "            sns.boxplot(x=data[col], ax=axs[index])\n",
    "            axs[index].set_title(f'Боксплот: {col}')\n",
    "            # тоже поворачиваем, если дискретные значения\n",
    "            if plot_type == 'dis':\n",
    "                axs[index].tick_params(axis='x', rotation=90)\n",
    "            index += 1\n",
    "\n",
    "    # Отключаем оставшиеся оси\n",
    "    for j in range(index, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def calc_target_correlations(df, target_col: str = None, drop_cols: list = None): # type: ignore\n",
    "    \"\"\"\n",
    "    Считает корреляции признаков с таргетом, строит heatmap и рассчитывает VIF.\n",
    "    Результаты выводятся прямо в Jupyter.\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    \n",
    "    df_tmp = df.copy()\n",
    "\n",
    "    # Преобразуем категориальные в числовые\n",
    "    cat_cols = df_tmp.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    for c in cat_cols:\n",
    "        df_tmp[c] = df_tmp[c].astype(\"category\").cat.codes\n",
    "\n",
    "    # Числовые колонки\n",
    "    numeric_cols = df_tmp.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
    "    if target_col not in numeric_cols:\n",
    "        raise ValueError(f\"target_col '{target_col}' должен быть числовым\")\n",
    "\n",
    "    # Корреляции с target\n",
    "    corr_df = (\n",
    "        df_tmp[numeric_cols]\n",
    "        .corr()[target_col]\n",
    "        .drop(target_col)\n",
    "        .sort_values(key=np.abs, ascending=False)\n",
    "    )\n",
    "    display(\"=== Корреляция с таргетом ===\")\n",
    "    display(corr_df)\n",
    "\n",
    "    # Heatmap\n",
    "    heatmap_cols = [col for col in numeric_cols if col not in drop_cols or col == target_col]\n",
    "    corr_matrix = df_tmp[heatmap_cols].corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(corr_matrix, interpolation=\"nearest\", cmap=\"coolwarm\", aspect=\"auto\")\n",
    "    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90, fontsize=8)\n",
    "    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns, fontsize=8)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Correlation Heatmap (включая target)\")\n",
    "\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        for j in range(corr_matrix.shape[1]):\n",
    "            value = corr_matrix.iloc[i, j]\n",
    "            plt.text(j, i, f\"{value:.2f}\", ha=\"center\", va=\"center\", fontsize=5, color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # VIF\n",
    "    vif_cols = [col for col in numeric_cols if col != target_col and col not in drop_cols]\n",
    "    X_vif = df_tmp[vif_cols].copy()\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X_vif), columns=vif_cols)\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = vif_cols\n",
    "    vif_data[\"VIF\"] = [\n",
    "        variance_inflation_factor(X_scaled.values, i) for i in range(X_scaled.shape[1])\n",
    "    ]\n",
    "    vif_data = vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "    display(\"=== VIF ===\")\n",
    "    display(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience=EARLY_STOP, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = -np.inf\n",
    "        self.no_improvement_count = 0\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        if study.best_value > self.best_score + self.min_delta:\n",
    "            self.best_score = study.best_value\n",
    "            self.no_improvement_count = 0\n",
    "        else:\n",
    "            self.no_improvement_count += 1\n",
    "\n",
    "        if self.no_improvement_count >= self.patience:\n",
    "            study.stop()\n",
    "            logger.info(f\"Ранняя остановка: нет улучшений {self.patience} trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c29023",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208ec76",
   "metadata": {},
   "source": [
    "### Подключим и почитаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531287cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_msg = pd.read_csv(\"../data/apparel-messages.csv\")\n",
    "app_prch = pd.read_csv(\"../data/apparel-purchases.csv\")\n",
    "app_target = pd.read_csv(\"../data/apparel-target_binary.csv\")\n",
    "event_type = pd.read_csv(\"../data/full_campaign_daily_event.csv\")\n",
    "event_chanel = pd.read_csv(\"../data/full_campaign_daily_event_channel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de2df2",
   "metadata": {},
   "source": [
    "### Первичная оценка и обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06984d6",
   "metadata": {},
   "source": [
    "#### app_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88eef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_msg = check_data(app_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим дату начала и конца событий\n",
    "first_date = app_msg['date'].min()\n",
    "last_date = app_msg['date'].max()\n",
    "display(first_date)\n",
    "display(last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f184c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим уники среди событий и канал распространения\n",
    "display(app_msg['event'].unique())\n",
    "display(app_msg['channel'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774b488",
   "metadata": {},
   "source": [
    "Что имеем:  \n",
    "open — письмо открыто  \n",
    "click — клик по ссылке в письме  \n",
    "purchase — покупка после перехода из письма  \n",
    "send — отправка письма  \n",
    "unsubscribe — отписка от рассылки  \n",
    "hbq_spam — сообщение отмечено как спам  \n",
    "hard_bounce — письмо не доставлено из-за постоянной ошибки (адрес не существует)  \n",
    "subscribe — подписка на рассылку  \n",
    "soft_bounce — письмо не доставлено из-за временной ошибки (ящик переполнен, сервер недоступен)  \n",
    "complain — жалоба пользователя (напр. “Это спам”)  \n",
    "close — завершение сессии (иногда: закрытие письма или вкладки)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7daa82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотри число дубликатов в базе\n",
    "app_msg.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0c04a",
   "metadata": {},
   "source": [
    "Вот тут тонкий момент:  \n",
    "1) Это косяк базы и надо просто удалить дубликаты и забыть про них;  \n",
    "2) Сомнения имею я, что это косяк и скорее всего это спамер просто отправил несколько мессаг и тогда из этого можно сделать бинарную фичу, что-то типа \"spam_factor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85960779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим сколько уникальных клиентов\n",
    "display(app_msg['client_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b33013",
   "metadata": {},
   "source": [
    "Из 12.739.798 строк мы имеем 53.329 уникальных клиентов.  \n",
    "Нужна будет пересборка данных с агрегацией"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44302e82",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "Самая объемная база.  \n",
    "Столбцы  date, created_at имеют неверный формат данных - необходимо будет преобразовать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15627b",
   "metadata": {},
   "source": [
    "#### app_prch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_prch = check_data(app_prch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим сколько уникальных клиентов совершило покупки\n",
    "display(app_prch['client_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe18213",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(app_prch['client_id'].nunique() / app_msg['client_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a376b7",
   "metadata": {},
   "source": [
    "Т.е. после всех событий произвели покупку 93.4% уникальных пользователей в текущей выборке, и почти 7% проигнорировало.  \n",
    "В целом это очень хороший показатель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d18d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# и еще посмотрим категории\n",
    "app_prch[app_prch['category_ids'] == '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0072da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут еще заодно проведем более глубокий анализ данных\n",
    "plot_combined(app_prch, col=None, target=None, col_type=None, legend_loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь нам интересно глянуть цены, сколько нулей и где\n",
    "app_prch[app_prch['price'] < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd577d2",
   "metadata": {},
   "source": [
    "Нулевых нет, уже хорошо.  \n",
    "Ну, а цена в одну единицу в целом бывает.  \n",
    "Также видим, что появились непонятные категории - пустые или None, что тоже не очень хорошо, но их обработает дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1603e",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "Столбец  date имеет неверный формат данных - необходимо будет преобразовать.  \n",
    "Столбец category_ids - тут 2 варианта развития событий:  \n",
    "- возьмем только глобальную категорию и конкретный товар, т.е. 1е и последнее значение;  \n",
    "- можно будет преобразовывать в разряженную матрица, т.к. небольшой объем данных  \n",
    "ну и потом с такой матрицей умеет работать xgboost который и будем использовать в предсказаниях;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76610a6f",
   "metadata": {},
   "source": [
    "#### app_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecec9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_target = check_data(app_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bf49b",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "Ну тут все понятно, обсуждать нечего"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded99126",
   "metadata": {},
   "source": [
    "#### event_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = check_data(event_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2664a7",
   "metadata": {},
   "source": [
    "Вывод:  \n",
    "Здесь просто агрегированная статистика по событиям, не вижу смысла, что-то делать с этой таблицей в принципе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b6ce8",
   "metadata": {},
   "source": [
    "#### event_chanel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52073b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_chanel = check_data(event_chanel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43826db",
   "metadata": {},
   "source": [
    "Вывод:  \n",
    "По сути своей, тоже самое, что и прошлая таблица - статистика по событиям.  \n",
    "Возможно и есть смысл где-то использовать, но пока непонятно где и как"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fab21",
   "metadata": {},
   "source": [
    "#### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0c3b0",
   "metadata": {},
   "source": [
    "Были подгружены и изучены предоставленные данные.  \n",
    "Глобально, для реализации задачи нам понадобится только 3 таблицы - apparel-messages, apparel-purchases и apparel-target_binary, т.к. эти таблицы несут основную смысловую нагрузку.  \n",
    "Две оставшиеся таблицы - статистика по ивентам и активности пользователей без привязки к этим самым пользователям и ничего нам не дадут.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720b136",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff9664",
   "metadata": {},
   "source": [
    "### Обработка имеющихся данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24053df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем даты\n",
    "app_msg[\"date\"] = pd.to_datetime(app_msg[\"date\"], errors=\"coerce\")\n",
    "app_msg[\"created_at\"] = pd.to_datetime(app_msg[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "app_prch[\"date\"] = pd.to_datetime(app_prch[\"date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d30de",
   "metadata": {},
   "source": [
    "### feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea7a0e",
   "metadata": {},
   "source": [
    "Для начала поработаем с каждой таблицей по отдельности и сделаем, что-то новое.  \n",
    "\n",
    "app_msg\n",
    "Что мы можем сделать:  \n",
    "1) У нас есть дата реакции на ивент и дата создания ивента - скорость реакции на ивент, потом усредняем;  \n",
    "2) Есть канал и действие - соберем суммарное число действий по каждому каналу;  \n",
    "3) Сделать бинарную фичу \"spam_factor\", т.е. берем дубли, смотрим кого заспамили - тем 1, кого нет - 0.  \n",
    "\n",
    "app_prch  \n",
    "Что мы можем сделать:  \n",
    "1) Время с момента последней покупки, сделаем в днях;  \n",
    "2) Среднее число товаров в заказе;  \n",
    "3) Средний чек заказа;  \n",
    "4) Средняя цена итема;  \n",
    "5) В какой категории больше всего покупок (предварительно разобьем category_ids на top и last id - глобальную категорию и конкретный товар);  \n",
    "6) Любимый товар;  \n",
    "7) Любимая категория;  \n",
    "8) Средний интервал между покупками, в днях;  \n",
    "9) Дней с последнего взаимодействия с рассылкой;  \n",
    "10) Сделаем группировку по 30/60/90/180/360 дней, а там агрегируем по числу покупок, числу итемов, сумме затрат от последней имеющейся у нас отчетной даты;  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a1324",
   "metadata": {},
   "source": [
    "#### app_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ead0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d55399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# У нас есть дата реакции на ивент и дата создания ивента - скорость реакции\n",
    "app_msg[\"reaction_for_event\"] = (\n",
    "    (app_msg[\"date\"].dt.normalize() - app_msg[\"created_at\"].dt.normalize())\n",
    "    .dt.days.astype(\"int16\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_msg['reaction_for_event'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ebacd",
   "metadata": {},
   "source": [
    "Неожиданно, все реагируют на ивенты в тот же день.  \n",
    "В таком случае этот признак для нас бесполезен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7990552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь агрегируем данные\n",
    "app_msg_agg = (\n",
    "    app_msg.groupby(\"client_id\")\n",
    "    .agg(\n",
    "        bulk_campaigns=(\"bulk_campaign_id\", \"nunique\"),\n",
    "        messages=(\"message_id\", \"nunique\"),\n",
    "        events=(\"event\", \"nunique\"),\n",
    "        channels=(\"channel\", \"nunique\"),\n",
    "        first_date=(\"date\", \"min\"),\n",
    "        last_date=(\"date\", \"max\"),\n",
    "        pop_event=(\"event\", lambda x: x.value_counts().idxmax() if not x.value_counts().empty else \"unknown\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем число действий по каналам и событиям\n",
    "channel_event_counts = (\n",
    "    pd.crosstab(\n",
    "        index=app_msg[\"client_id\"],\n",
    "        columns=[app_msg[\"channel\"], app_msg[\"event\"]]\n",
    "    )\n",
    ")\n",
    "\n",
    "# делаем имена колонок: actions_{channel}_{event}\n",
    "channel_event_counts.columns = [\n",
    "    f\"actions_{ch}_{ev}\" for ch, ev in channel_event_counts.columns\n",
    "]\n",
    "\n",
    "# собираем в кучу\n",
    "channel_event_counts = channel_event_counts.reset_index()\n",
    "app_msg_agg = app_msg_agg.merge(channel_event_counts, on=\"client_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f227a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем количество дублей сообщений на юзера\n",
    "dup_counts = (\n",
    "    app_msg.groupby(\"client_id\")\n",
    "    .agg(dup_count=(\"message_id\", lambda x: app_msg.loc[x.index].duplicated().sum()))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# spam_factor = 1 если дублей >= 2, иначе 0\n",
    "dup_counts[\"spam_factor\"] = (dup_counts[\"dup_count\"] >= 2).astype(int)\n",
    "\n",
    "# объединяем с агрегатами\n",
    "app_msg_agg = app_msg_agg.merge(dup_counts, on=\"client_id\", how=\"left\")\n",
    "app_msg_agg[[\"dup_count\", \"spam_factor\"]] = app_msg_agg[[\"dup_count\", \"spam_factor\"]].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_msg_agg[app_msg_agg[\"spam_factor\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c210a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(app_msg_agg.head())\n",
    "display(app_msg_agg.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6a03a",
   "metadata": {},
   "source": [
    "Выглядит жутко, но мы закрываем глаза и продолжаем делать :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553fb32",
   "metadata": {},
   "source": [
    "#### app_prch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5725b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# начнем с разбора category_ids\n",
    "# превращаем в списки category_ids\n",
    "app_prch[\"category_ids\"] = app_prch[\"category_ids\"].apply(parse_category_ids)\n",
    "\n",
    "# берём первую категорию\n",
    "app_prch[\"category_ids_top\"] = app_prch[\"category_ids\"].apply(\n",
    "    lambda x: int(x[0]) if len(x) > 0 and str(x[0]).isdigit() else -1\n",
    ").astype(\"int16\")\n",
    "\n",
    "# берём последнюю непустую категорию\n",
    "app_prch[\"category_ids_last\"] = app_prch[\"category_ids\"].apply(\n",
    "    lambda x: int(next((i for i in reversed(x) if i not in [None, \"\", \"nan\"] and str(i).isdigit()), -1))\n",
    ").astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# время с момента последней покупки (в днях)\n",
    "last_date = pd.to_datetime(last_date)\n",
    "\n",
    "last_purchase = (\n",
    "    app_prch.groupby(\"client_id\")[\"date\"].max()\n",
    "    .reset_index(name=\"last_purchase_date\")\n",
    ")\n",
    "\n",
    "last_purchase[\"days_from_last_purchase\"] = (\n",
    "    (last_date - last_purchase[\"last_purchase_date\"]).dt.days.astype(\"int16\")\n",
    ")\n",
    "last_purchase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d964f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее число товаров в заказе\n",
    "avg_items_per_order = (\n",
    "    app_prch.groupby([\"client_id\", \"message_id\"])[\"quantity\"].sum()\n",
    "    .groupby(\"client_id\").median()\n",
    "    .reset_index(name=\"avg_items_per_order\")\n",
    ")\n",
    "avg_items_per_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# средний чек заказа\n",
    "avg_order_sum = (\n",
    "    app_prch.groupby([\"client_id\", \"message_id\"])\n",
    "    .apply(lambda df: (df[\"quantity\"] * df[\"price\"]).sum(), include_groups=False)\n",
    "    .groupby(\"client_id\").mean()\n",
    "    .reset_index(name=\"avg_order_sum\")\n",
    ")\n",
    "avg_order_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05827522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# средняя цена итема\n",
    "avg_item_price = (\n",
    "    (app_prch[\"price\"] * app_prch[\"quantity\"])\n",
    "    .groupby(app_prch[\"client_id\"]).sum()\n",
    "    / app_prch.groupby(\"client_id\")[\"quantity\"].sum()\n",
    ")\n",
    "avg_item_price = avg_item_price.reset_index(name=\"avg_item_price\")\n",
    "avg_item_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# любимая категория (top)\n",
    "fav_category_top = (\n",
    "    app_prch.groupby(\"client_id\")[\"category_ids_top\"]\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else \"unknown\")\n",
    "    .reset_index(name=\"fav_category_top\")\n",
    ")\n",
    "fav_category_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# любимый товар (last)\n",
    "fav_category_last = (\n",
    "    app_prch.groupby(\"client_id\")[\"category_ids_last\"]\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else \"unknown\")\n",
    "    .reset_index(name=\"fav_category_last\")\n",
    ")\n",
    "fav_category_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef894af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# средний интервал между покупками (в днях)\n",
    "avg_interval = (\n",
    "    app_prch.groupby(\"client_id\")[\"date\"]\n",
    "    .apply(\n",
    "        lambda x: np.mean(np.diff(np.sort(x.unique())).astype(\"timedelta64[D]\").astype(int))\n",
    "        if x.nunique() > 1 else -1\n",
    "    )\n",
    "    .reset_index(name=\"avg_interval_days\")\n",
    ")\n",
    "display(avg_interval.sample(5))\n",
    "display(avg_interval.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ed9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сумма покупок\n",
    "lifetime_value = (\n",
    "    (app_prch[\"quantity\"] * app_prch[\"price\"])\n",
    "    .groupby(app_prch[\"client_id\"]).sum()\n",
    "    .reset_index(name=\"lifetime_value\")\n",
    ")\n",
    "display(lifetime_value.head())\n",
    "display(lifetime_value.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cf24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn-флаг (>360 дней без покупок)\n",
    "churn_flag = (\n",
    "    app_prch.groupby(\"client_id\")[\"date\"].max()\n",
    "    .reset_index(name=\"last_purchase_date\")\n",
    ")\n",
    "churn_flag[\"churn_flag\"] = (\n",
    "    (last_date - churn_flag[\"last_purchase_date\"]).dt.days > 360\n",
    ").astype(\"int8\")\n",
    "churn_flag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77849e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# агрегации по окнам 30/60/90/180/360 дней\n",
    "def agg_period(df, cutoff_days, all_clients):\n",
    "    cutoff = last_date - pd.Timedelta(days=cutoff_days)\n",
    "    dff = df[df[\"date\"] >= cutoff]\n",
    "    out = dff.groupby(\"client_id\").agg(\n",
    "        purchases=(\"message_id\", \"nunique\"),\n",
    "        items=(\"quantity\", \"sum\"),\n",
    "        cost=(\"price\", lambda x: (dff.loc[x.index, \"quantity\"] * x).sum())\n",
    "    )\n",
    "\n",
    "    out = out.rename(columns={\n",
    "        \"purchases\": f\"purchases_{cutoff_days}d\",\n",
    "        \"items\": f\"items_{cutoff_days}d\",\n",
    "        \"cost\": f\"cost_{cutoff_days}d\"\n",
    "    })\n",
    "\n",
    "    # добавляем всех клиентов, которых нет в dff\n",
    "    out = out.reindex(all_clients, fill_value=0).reset_index()\n",
    "    return out\n",
    "\n",
    "all_clients = app_prch[\"client_id\"].unique()\n",
    "agg_30 = agg_period(app_prch, 30, all_clients)\n",
    "agg_60 = agg_period(app_prch, 60, all_clients)\n",
    "agg_90 = agg_period(app_prch, 90, all_clients)\n",
    "agg_180 = agg_period(app_prch, 180, all_clients)\n",
    "agg_360 = agg_period(app_prch, 360, all_clients)\n",
    "\n",
    "agg_360.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединяем все в единый датафрейм client_features\n",
    "app_prch_agg = last_purchase[[\"client_id\", \"days_from_last_purchase\"]] \\\n",
    "    .merge(avg_items_per_order, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(avg_order_sum, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(avg_item_price, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(fav_category_top, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(fav_category_last, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(avg_interval, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(lifetime_value, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(churn_flag[[\"client_id\", \"churn_flag\"]], on=\"client_id\", how=\"left\") \\\n",
    "    .merge(agg_30, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(agg_60, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(agg_90, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(agg_180, on=\"client_id\", how=\"left\") \\\n",
    "    .merge(agg_360, on=\"client_id\", how=\"left\")\n",
    "\n",
    "app_prch_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84749e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_prch_agg['client_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_msg_agg['client_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273c075",
   "metadata": {},
   "source": [
    "### Финальная таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cdb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ну и финально смержим 2 агрегированные таблицы и таблицу с таргетом\n",
    "df = app_prch_agg.merge(app_msg_agg, on='client_id', how='left')\n",
    "df = df.merge(app_target, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8573fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1698672",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценим пропуски\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909264f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# как оказалось списки клиентов не бьются в файла и есть неучтенные\n",
    "# столбцы для заполнения \"unknown\"\n",
    "cols_unknown = [\"first_date\", \"last_date\", \"pop_event\"]\n",
    "\n",
    "# все остальные перечисленные числовые столбцы\n",
    "cols_numeric = [\n",
    "    \"bulk_campaigns\", \"messages\", \"events\", \"channels\",\n",
    "    \"actions_email_click\", \"actions_email_complain\", \"actions_email_hard_bounce\",\n",
    "    \"actions_email_hbq_spam\", \"actions_email_open\", \"actions_email_purchase\",\n",
    "    \"actions_email_send\", \"actions_email_soft_bounce\", \"actions_email_subscribe\",\n",
    "    \"actions_email_unsubscribe\", \"actions_mobile_push_click\", \"actions_mobile_push_close\",\n",
    "    \"actions_mobile_push_hard_bounce\", \"actions_mobile_push_open\", \"actions_mobile_push_purchase\",\n",
    "    \"actions_mobile_push_send\", \"actions_mobile_push_soft_bounce\",\n",
    "    \"dup_count\", \"spam_factor\"\n",
    "]\n",
    "\n",
    "# заменяем пропуски\n",
    "df[cols_unknown] = df[cols_unknown].fillna(\"unknown\")\n",
    "df[cols_numeric] = df[cols_numeric].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de051184",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in zip(df.columns, df.dtypes):\n",
    "    display(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abb468",
   "metadata": {},
   "source": [
    "### Корелляция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица корелляции получилась дикой, поэтому сразу буду прописывать в дропы после last_date фичи которые коррелируют в единицу с чем-либо без дополнительных выводов + vif сюда же\n",
    "# с точки зрения бизнеса я конечно же не прав, но и захламлять не вижу смысла файл\n",
    "drop_cols = ['client_id', 'first_date', 'last_date', 'messages', 'bulk_campaigns', 'cost_90d', 'items_90d', 'churn_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65375a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_train = df.drop(columns=drop_cols)\n",
    "calc_target_correlations(df_for_train, target_col=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd18708",
   "metadata": {},
   "source": [
    "vif у этих фич получился не очень... будем убирать по одному (выше уже будет убранный вариант)  \n",
    "25\tmessages\t4751.468827045559  \n",
    "24\tbulk_campaigns\t4557.404604056862  \n",
    "44\tactions_mobile_push_send\t86.63033251646662  \n",
    "35\tactions_email_send\t37.31511841292837  \n",
    "17\tcost_90d\t32.38699214915867  \n",
    "14\tcost_60d\t27.655784671292132  \n",
    "8\tchurn_flag\t23.014173540405533  \n",
    "16\titems_90d\t21.396144750100234  \n",
    "13\titems_60d\t18.35091376893147  \n",
    "15\tpurchases_90d\t17.067579757645326  \n",
    "12\tpurchases_60d\t14.87017587054433  \n",
    "21\tpurchases_360d\t13.347259333650094  \n",
    "20\tcost_180d\t10.067449835369827  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6835f6",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46528cf8",
   "metadata": {},
   "source": [
    "### Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем часть данных для подбора гиперпараметров и финального теста\n",
    "df_test = df_for_train.sample(frac=0.05, random_state=RANDOM_STATE)\n",
    "df_rest = df_for_train.drop(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим на выборки из оставшихся данных\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_rest.drop(['target'], axis=1),\n",
    "    df_rest['target'],\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df_rest['target'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# финальный отложенный тест\n",
    "X_final_test = df_test.drop(['target'], axis=1)\n",
    "y_final_test = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сюда будем писать результаты\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем признаки\n",
    "cat_selector = make_column_selector(dtype_include=[\"object\", \"category\"])\n",
    "num_selector = make_column_selector(dtype_exclude=[\"object\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = cat_selector(X_train)\n",
    "num_cols = num_selector(X_train)\n",
    "\n",
    "display(\"Категориальные:\", cat_cols)\n",
    "display(\"Числовые:\", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str = FunctionTransformer(lambda x: x.astype(str))\n",
    "\n",
    "# для линейных моделей\n",
    "preprocessor_linear = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_selector),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"to_str\", to_str),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False))\n",
    "        ]), cat_selector)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# для деревьев и бустингов\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_selector),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"to_str\", to_str),\n",
    "            (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "        ]), cat_selector)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be765c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": (lambda: LogisticRegression(random_state=RANDOM_STATE), preprocessor_linear),\n",
    "    \"RandomForest\": (lambda: RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS), preprocessor_tree),\n",
    "    \"DecisionTree\": (lambda: DecisionTreeClassifier(random_state=RANDOM_STATE), preprocessor_tree),\n",
    "    \"LightGBM\": (lambda: LGBMClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS, verbosity=-1), preprocessor_tree),\n",
    "    \"XGBoost\": (lambda: XGBClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS, verbosity=0, use_label_encoder=False), preprocessor_tree),\n",
    "    \"CatBoost\": (lambda: CatBoostClassifier(random_state=RANDOM_STATE, task_type=\"CPU\", thread_count=N_JOBS, verbose=0), preprocessor_tree),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_model(trial, model_name, preprocessor):\n",
    "    # разделяем train/val внутри trial\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.25,\n",
    "        stratify=y_train,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # число признаков для SelectKBest\n",
    "    k_best = trial.suggest_int(\"selectkbest__k\", 5, X_train.shape[1])\n",
    "\n",
    "    # настраиваем гиперпараметры конкретной модели\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        C = trial.suggest_float(\"C\", 0.01, 10, log=True)\n",
    "        model = LogisticRegression(C=C, random_state=RANDOM_STATE, max_iter=1000)\n",
    "    elif model_name == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                       random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, random_state=RANDOM_STATE)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=n_estimators, max_depth=max_depth,\n",
    "            learning_rate=learning_rate, random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS, verbosity=-1\n",
    "        )\n",
    "    elif model_name == \"XGBoost\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators, max_depth=max_depth,\n",
    "            learning_rate=learning_rate, random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS, verbosity=0, use_label_encoder=False\n",
    "        )\n",
    "    elif model_name == \"CatBoost\":\n",
    "        iterations = trial.suggest_int(\"iterations\", 100, 1000, step=50)\n",
    "        depth = trial.suggest_int(\"depth\", 3, 10)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=iterations, depth=depth,\n",
    "            learning_rate=learning_rate, random_state=RANDOM_STATE,\n",
    "            task_type=\"CPU\", thread_count=N_JOBS, verbose=0\n",
    "        )\n",
    "\n",
    "    # pipeline с SelectKBest\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"selectkbest\", SelectKBest(score_func=f_classif, k=k_best)),\n",
    "        (\"classifier\", model)\n",
    "    ])\n",
    "\n",
    "    # обучаем\n",
    "    if model_name in [\"LightGBM\", \"XGBoost\", \"CatBoost\"]:\n",
    "        # Для бустингов — используем eval_set и report для pruner\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "        y_pred = pipeline.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "        y_pred = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    # пробуем передать в Optuna значение для прунинга\n",
    "    trial.report(auc, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "study_results = []\n",
    "\n",
    "early_stop_cb = EarlyStoppingCallback(patience=EARLY_STOP)\n",
    "\n",
    "def log_every_N_trials(study, trial):\n",
    "    if trial.number % 100 == 0:\n",
    "        logger.info(f\"[Trial {trial.number}] ROC_AUC={trial.value:.4f}, params={trial.params}\")\n",
    "\n",
    "for model_name, (model_factory, preprocessor) in models.items():\n",
    "    logger.info(f\"\\n\\n=== Optimizing {model_name} ===\")\n",
    "    \n",
    "    # pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective_model(trial, model_name, preprocessor),\n",
    "        n_trials=N_ITER,\n",
    "        callbacks=[log_every_N_trials, early_stop_cb]\n",
    "    )\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "\n",
    "    logger.info(f\"Best trial -> ROC_AUC={best_value:.4f}, params={best_params}\")\n",
    "\n",
    "    k_best_final = best_params.get(\"selectkbest__k\", X_train.shape[1])\n",
    "    final_pipe = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"selectkbest\", SelectKBest(score_func=f_classif, k=k_best_final)),\n",
    "        (\"classifier\", model_factory())\n",
    "    ])\n",
    "\n",
    "    model_params = {k.replace(\"classifier__\", \"\"): v for k, v in best_params.items() if k.startswith(\"classifier__\")}\n",
    "    if model_params:\n",
    "        final_pipe.named_steps[\"classifier\"].set_params(**model_params)\n",
    "\n",
    "    final_pipe.fit(X_train, y_train)\n",
    "    best_models[model_name] = final_pipe\n",
    "\n",
    "    study_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best_params\": best_params,\n",
    "        \"ROC_AUC_CV\": best_value\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04ccf5",
   "metadata": {},
   "source": [
    "### Подбор лучшей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5973a",
   "metadata": {},
   "source": [
    "### Сравнение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим колонки для ROC_AUC на X_val и X_final_test\n",
    "for res in study_results:\n",
    "    name = res[\"Model\"]\n",
    "    model = best_models[name]\n",
    "\n",
    "    # Предсказания вероятностей для положительного класса\n",
    "    y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    y_test_pred = model.predict_proba(X_final_test)[:, 1]\n",
    "\n",
    "    # Вычисляем ROC AUC\n",
    "    res[\"ROC_AUC_Val\"] = roc_auc_score(y_val, y_val_pred)\n",
    "    res[\"ROC_AUC_FinalTest\"] = roc_auc_score(y_final_test, y_test_pred)\n",
    "\n",
    "# Пересоздаём DataFrame с обновлёнными результатами\n",
    "results_df = pd.DataFrame(study_results)\n",
    "display(results_df.sort_values(\"ROC_AUC_FinalTest\", ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
