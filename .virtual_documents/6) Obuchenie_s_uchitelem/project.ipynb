























# # здесь будем ставить все, чего нам не хватает
# !pip install --upgrade pip -q
# !pip install missingno -q
# !pip install optuna -q
# !pip install optuna-integration[sklearn] -q
# !pip install shap -q
# !pip install phik -q
# !pip install --upgrade scikit-learn -q
# !pip install tdqm -q
# !pip install imbalanced-learn -q


# а здесь импортировать все для работы
import warnings

import itertools

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import sklearn
import optuna
import plotly.express as px
import plotly.graph_objects as go

from IPython.display import (
    display,
    HTML
)
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import (
    LogisticRegression,
    LinearRegression
)
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    f1_score
)
from sklearn.model_selection import (
    train_test_split,
    GridSearchCV,
    RandomizedSearchCV,
    cross_val_score
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import (
    Pipeline,
    FeatureUnion
)
from sklearn.preprocessing import (
    OneHotEncoder,
    OrdinalEncoder,
    StandardScaler,
    MinMaxScaler,
    RobustScaler,
    LabelEncoder
)
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.inspection import permutation_importance
from optuna.integration import OptunaSearchCV
from optuna.distributions import (
    IntDistribution,
    FloatDistribution,
    CategoricalDistribution
)
from phik import phik_matrix
from statsmodels.stats.outliers_influence import variance_inflation_factor
from plotly.subplots import make_subplots
from tqdm import tqdm
from joblib import parallel_backend
from imblearn.over_sampling import SMOTE


# это даст нам красивые выводы
sns.set()
pd.set_option('display.float_format', lambda x: '%.2f' % x)
warnings.simplefilter(action='ignore', category=FutureWarning)


# здесь и дальше подготовим функции которые пригодятся в дальнейшем


# сделаем функцию оценки пропусков в датасетах
def missing_data(data):
    missing_data = data.isna().sum()
    missing_data = missing_data[missing_data > 0]
    display(missing_data)


# функция для обработки пробелов
def process_spaces(s):
    if isinstance(s, str):
        s = s.strip()
        s = ' '.join(s.split())
    return s


# замена пробелов на нижнее подчеркинвание в названии столбцов
def replace_spaces(s):
    if isinstance(s, str):
        s = s.strip()
        s = '_'.join(s.split())
    return s


def drop_duplicated(data):
    # проверка дубликатов
    display(format_display("Проверим дубликаты и удалим, если есть"))
    num_duplicates = data.duplicated().sum()
    display(num_duplicates)
    
    if num_duplicates > 0:
        display("Удаляем")
        data = data.drop_duplicates(keep='first').reset_index(drop=True)  # обновляем DataFrame
    else:
        display("Дубликаты отсутствуют")
    return data


# форматирования текста
def format_display(text):
    return HTML(f"<span style='font-size: 1.5em; font-weight: bold; font-style: italic;'>{text}</span>")


def check_data(data):
    # приведем все к нижнему регистру
    data.columns = data.columns.str.lower()
    
    # удалим лишние пробелы в строках
    data = data.applymap(process_spaces)

    # и в названии столбцов
    data.columns = [replace_spaces(col) for col in data.columns]
    
    # общая информация 
    display(format_display("Общая информация базы данных"))
    display(data.info())
    
    # 5 строк
    display(format_display("5 случайных строк"))
    display(data.sample(5))
    
    # пропуски
    display(format_display("Число пропусков в базе данных"))
    display(missing_data(data))

    # проверка на наличие пропусков
    if data.isnull().sum().sum() > 0:
        display(format_display("Визуализация пропусков"))
        msno.bar(data)
        plt.show()
        
    # средние характеристики
    display(format_display("Характеристики базы данных"))
    display(data.describe().T)
    
    data = drop_duplicated(data)
    
    return data  # возвращаем измененные данные


def check_unique_cat(data):
    text_columns = data.select_dtypes(include=['object'])
    
    if text_columns.empty:
        display("В DataFrame нет столбцов с данными типа 'object'.")
        return
    
    unique_values = {col: text_columns[col].unique() for col in text_columns.columns}
    for col, values in unique_values.items():
        display(f"Уникальные значения в столбце '{col}': {values}")


def check_numeric_columns(data):
    numeric_columns = data.select_dtypes(include=['int64', 'float64'])
    
    if numeric_columns.empty:
        return []
    else:
        return numeric_columns.columns.tolist()


# боксплот - только столбцы типа int и float
def plot_numerical_columns(data):
    numerical_columns = data.select_dtypes(include=['int', 'float']).columns.tolist()
    
    n = len(numerical_columns)
    ncols = 3
    nrows = (n + ncols - 1) // ncols

    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 5 * nrows))
    axs = axs.flatten()

    for i, col in enumerate(numerical_columns):
        axs[i].boxplot(data[col].dropna())
        axs[i].set_title(col)
        axs[i].set_ylabel('Значения')
        axs[i].set_xticklabels([''])

    for j in range(i + 1, len(axs)):
        axs[j].axis('off')

    plt.tight_layout()
    plt.show()


def plot_numerical_columns_hist(data, target_column=None):
    numerical_columns = data.select_dtypes(include=['int', 'float']).columns.tolist()
    num_cols = len(numerical_columns)
    
    ncols = 2
    nrows = int(np.ceil(num_cols / ncols))

    f, axes = plt.subplots(nrows, ncols, figsize=(16, nrows * 4))
    axes = axes.flatten()

    for i, col in enumerate(numerical_columns):
        # Гистограмма
        axes[i].set_title(f'Распределение признака {col}', fontsize=16)
        axes[i].set_ylabel('Количество', fontsize=14)

        if target_column and target_column in data.columns:
            sns.histplot(data=data, x=col, bins=20, kde=True, ax=axes[i], hue=target_column, multiple="stack")
        else:
            sns.histplot(data=data, x=col, bins=20, kde=True, ax=axes[i])

    for j in range(i + 1, nrows * ncols):
        f.delaxes(axes[j])

    plt.tight_layout()
    plt.show()


def plot_categorical_columns(data, col=None):
    categorical_columns = data.select_dtypes(include=['object']).columns.tolist()
    
    if col is not None and col in categorical_columns:
        categorical_columns = [col]
    elif col is not None:
        print(f"Столбец '{col}' не найден в данных.")
        return

    n = len(categorical_columns)
    ncols = 2
    nrows = (n * 2 + ncols - 1) // ncols

    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6 * nrows))
    axs = axs.flatten()

    index = 0
    for col in categorical_columns:
        # круговая диаграмма
        data[col].value_counts().plot(kind='pie', ax=axs[index], autopct='%1.1f%%', startangle=90)
        axs[index].set_title(f'{col}')
        axs[index].set_ylabel('')
        index += 1

        # гистограмма
        data[col].value_counts().plot(kind='bar', ax=axs[index])
        axs[index].set_title(f'{col}')
        axs[index].set_ylabel('Частота')
        index += 1

    for j in range(index, len(axs)):
        axs[j].axis('off')

    plt.tight_layout()
    plt.show()



def plot_combined(data, target=None, legend_loc='best'):
    numerical_columns = data.select_dtypes(include=['int', 'float']).columns.tolist()

    total_plots = len(numerical_columns) * 2
    ncols = 2
    nrows = (total_plots + ncols - 1) // ncols

    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 5 * nrows))
    axs = axs.flatten()

    index = 0

    for col in numerical_columns:
        # гистограмма
        if index < len(axs):
            if target is not None:
                sns.histplot(data, x=col, hue=target, bins=20, kde=True, ax=axs[index])
                handles, labels = axs[index].get_legend_handles_labels()
                if handles:
                    axs[index].legend(title=target, loc=legend_loc)
            else:
                sns.histplot(data[col].dropna(), bins=20, kde=True, ax=axs[index])
            axs[index].set_title(f'Гистограмма: {col}')
            index += 1

        # боксплот
        if index < len(axs):
            sns.boxplot(x=data[col], ax=axs[index])
            axs[index].set_title(f'Боксплот: {col}')
            index += 1

    for j in range(index, len(axs)):
        axs[j].axis('off')

    plt.tight_layout()
    plt.show()


def merge_base(bases, index, merge_type):
    """
    Объединяет список таблиц по заданному индексу.

    :param bases: Список таблиц (DataFrame), которые нужно объединить.
    :param index: Имя столбца, по которому будет происходить объединение.
    :param merge_type: Тип объединения ('inner', 'outer', 'left', 'right').
    :return: Объединённая таблица.
    """
    merged_df = bases[0]
    for base in bases[1:]:
        merged_df = pd.merge(merged_df, base, on=index, how=merge_type)

    return merged_df


def get_ohe_columns(data):
    # выбор бинарных признаков (категориальные с 2 уникальными значениями)
    return data.select_dtypes(include=['object']).nunique()[data.select_dtypes(include=['object']).nunique() == 2].index.tolist()


def get_ord_columns(data):
    # выбор категориальных признаков (с более чем 2 уникальными значениями)
    return data.select_dtypes(include=['object']).nunique()[data.select_dtypes(include=['object']).nunique() > 2].index.tolist()


def get_num_columns(data):
    # используем уже созданную функцию для получения количественных признаков
    return check_numeric_columns(data)


def plot_scatter_with_categories(data):
    """
    Функция для построения диаграмм рассеяния с разбивкой по категориальным данным.
    
    :param data: DataFrame с данными.
    """
    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for column in categorical_columns:
        plt.figure(figsize=(15, 10))
        sns.scatterplot(data=data, x='вероятность_снижения', y='прибыль', hue=column, alpha=0.7)
        plt.xlabel('Вероятность снижения активности')
        plt.ylabel('Прибыль')
        plt.title(f'Зависимость вероятности снижения активности от прибыли по {column}')
        plt.legend(title=column)
        plt.show()


def histogram(data, col, target):
    plt.figure(figsize=(8,6))
    plot = sns.histplot(data, bins=20, kde=True, hue=target, x=col)
    plot.set_title(f'Рапределение по {col}', fontsize=16)
    plot.set_ylabel('Количество', fontsize=14)


def plot_density_heatmap(data, x, y):
    fig = px.density_heatmap(data, x=x, y=y, nbinsx=50, nbinsy=50, color_continuous_scale='Greens')
    fig.update_layout(title=f'Зависимость {x} от {y}',
                      xaxis_title=x,
                      yaxis_title=y,
                      width=600,
                      height=500)
    fig.show()





# Таблица, которая содержит данные о поведении покупателя на сайте,
# о коммуникациях с покупателем и его продуктовом поведении.
market_file = pd.read_csv('https://code.s3.yandex.net/datasets/market_file.csv')


# Таблица с данными о выручке, которую получает магазин с покупателя,
# то есть сколько покупатель всего потратил за период взаимодействия с сайтом.
market_money = pd.read_csv('https://code.s3.yandex.net/datasets/market_money.csv')


# Таблица с данными о времени (в минутах), которое покупатель провёл на сайте в течение периода.
market_time = pd.read_csv('https://code.s3.yandex.net/datasets/market_time.csv')


# Таблица с данными о среднемесячной прибыли продавца за последние 3 месяца:
# какую прибыль получает магазин от продаж каждому покупателю.
money = pd.read_csv('https://code.s3.yandex.net/datasets/money.csv', delimiter=';', decimal = ',')





# посмотрим, что у нас в каждом файле имеется
# обработкой, доработкой и преобразованиями займемся позже
market_file = check_data(market_file)





market_money = check_data(market_money)





market_time = check_data(market_time)





money = check_data(money)














# изменение типов данных в market_file
market_file['маркет_актив_6_мес'] = market_file['маркет_актив_6_мес'].astype(float)
market_file['акционные_покупки'] = market_file['акционные_покупки'].astype(float)

# изменение типов данных в market_money
market_money['выручка'] = market_money['выручка'].astype(float)

# изменим значения
money['прибыль'] = money['прибыль'] * 1000


# т.к. нам пришлось менять тип данных в не которых позициях, посмотрим еще раз на статистику
market_file.describe().T





money.describe().T





market_money.describe().T


# а вот тут не очень, почему-то минимальная выручка 0 (да, продавец может уйти и в минус, и не продать ничего, надо проверить)
display(market_money[market_money['выручка'] == 0])
# и есть очень большая выручка в 106862.20, а у нас таких значений в прибыли не наблюдается
display(market_money[market_money['выручка'] > 10000])


# исправим на медианное значение, явно ошибка, т.к. среднее значение всего ~5000
median_value = market_money[market_money['выручка'] < 10000]['выручка'].median()
market_money.loc[market_money['выручка'] > 10000, 'выручка'] = median_value








check_unique_cat(market_file)


# исправим "стандартт" и "...аксесуары"
market_file.loc[market_file['тип_сервиса'] == 'стандартт', 'тип_сервиса'] = 'стандарт'
market_file.loc[market_file['популярная_категория'] == 'Косметика и аксесуары', 'популярная_категория'] = 'Косметика и аксессуары'


check_unique_cat(market_money)


check_unique_cat(market_time)


# ну тоже, как то странно выглядит.... исправим предыдцщий_месяц на предыдущий_месяц
market_time.loc[market_time['период'] == 'предыдцщий_месяц', 'период'] = 'предыдущий_месяц'


check_unique_cat(money)











base = market_file.drop(columns=['id'])


plot_combined(base, 'покупательская_активность')








plot_categorical_columns(market_file)








base = market_money.drop(columns=['id'])
grp = pd.DataFrame(base.groupby('период')['выручка'].sum())
plot_combined(base)


ordered_periods = ['препредыдущий_месяц', 'предыдущий_месяц', 'текущий_месяц']
base['период'] = pd.Categorical(base['период'], categories=ordered_periods, ordered=True)

grouped_data = base.groupby('период')['выручка'].sum().reset_index()
plt.figure(figsize=(10, 6))

bars = plt.bar(grouped_data['период'], grouped_data['выручка'], label='Суммарная выручка')

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), 
             ha='center', va='bottom')

plt.plot(grouped_data['период'], grouped_data['выручка'], color='red', marker='o', label='Кривая выручки')

plt.xlabel('Период')
plt.ylabel('Суммарная выручка')
plt.title('Суммарная выручка по периодам')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()


base['период'] = pd.Categorical(base['период'], categories=ordered_periods, ordered=True)

plt.figure(figsize=(15, 10))

for i, period in enumerate(ordered_periods):
    plt.subplot(len(ordered_periods), 1, i + 1)
    subset = base[base['период'] == period]
    plt.hist(subset['выручка'], bins=50, alpha=0.7)
    plt.title(f'Гистограмма выручки для периода: {period}')
    plt.xlabel('Выручка')
    plt.ylabel('Частота')

plt.tight_layout()
plt.show()





base['период'] = pd.Categorical(base['период'], categories=ordered_periods, ordered=True)

plt.figure(figsize=(10, 6))

sns.histplot(data=base, x='выручка', hue='период', multiple='stack', bins=70, alpha=0.7, edgecolor='black')

plt.title('Гистограмма выручки по периодам')
plt.xlabel('Выручка')
plt.ylabel('Частота')

plt.show()





# Отберите клиентов с покупательской активностью не менее трёх месяцев,
# то есть таких, которые что-либо покупали в этот период.
# нам нужна таблица market_money, где помесячно указана выручка от каждого клиета
# посчитаем...

active_clients = market_money.groupby(['id', 'период']).sum().reset_index()

valid_ids = active_clients.groupby('id').filter(lambda x: (x['выручка'] != 0).all())

unique_ids = len(market_money['id'].unique())
valid_unique_ids = len(valid_ids['id'].unique())

market_money = market_money[market_money['id'].isin(valid_ids['id'])]
display(f"Общее количество уникальных id: {unique_ids}")
display(f"Количество id с выручкой всегда больше 0: {valid_unique_ids}")








base = market_time.drop(columns=['id'])
plot_combined(base)


ordered_periods = ['предыдущий_месяц', 'текущий_месяц']
base['период'] = pd.Categorical(base['период'], categories=ordered_periods, ordered=True)

grouped_data = base.groupby('период')['минут'].sum().reset_index()
plt.figure(figsize=(10, 6))

bars = plt.bar(grouped_data['период'], grouped_data['минут'], label='Суммарное время')

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), 
             ha='center', va='bottom')  # Подпись над столбцом

plt.plot(grouped_data['период'], grouped_data['минут'], color='red', marker='o', label='Кривая времени')

plt.xlabel('Период')
plt.ylabel('Суммарное время')
plt.title('Суммарное время по периодам')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()


base['период'] = pd.Categorical(base['период'], categories=ordered_periods, ordered=True)

plt.figure(figsize=(15, 10))

for i, period in enumerate(ordered_periods):
    plt.subplot(len(ordered_periods), 1, i + 1)
    subset = base[base['период'] == period]
    plt.hist(subset['минут'], bins=20, alpha=0.7)
    plt.title(f'Гистограмма выручки для периода: {period}')
    plt.xlabel('Минуты')
    plt.ylabel('Частота')

plt.tight_layout()
plt.show()


base['период'] = pd.Categorical(base['период'], categories=ordered_periods, ordered=True)

plt.figure(figsize=(10, 6))

sns.histplot(data=base, x='минут', hue='период', multiple='stack', bins=40, alpha=0.7, edgecolor='black')

plt.title('Гистограмма времени по периодам')
plt.xlabel('Минуты')
plt.ylabel('Частота')

plt.show()








base = money.drop(columns=['id'])
plot_combined(base)




















market_money_grouped = market_money.pivot_table(index=['id'], columns=["период"])
market_money_grouped.columns = ['выручка_предыдущий_месяц', 'выручка_препредыдущий_месяц', 'выручка_текущий_месяц']
market_money_grouped['id'] = market_money_grouped.index


market_time_grouped = market_time.pivot_table(index=['id'], columns=["период"])
market_time_grouped.columns = ['минут_предыдущий_месяц', 'минут_текущий_месяц']
market_time_grouped['id'] = market_time_grouped.index


market_money_grouped = market_money_grouped.rename(columns={'id': 'id_money'})
market_time_grouped = market_time_grouped.rename(columns={'id': 'id_time'})
market = market_file.join(market_money_grouped, on='id', lsuffix="_left", rsuffix="_выручка")
market = market.rename(columns={'id_left': 'id'})
market = market.join(market_time_grouped, on='id', lsuffix="_left", rsuffix="_минут")
market = market.rename(columns={'id_left': 'id'})
market = market.drop(['id_money', 'id_time'], axis=1)


# помним, что у нас есть 3 клиента которых надо убрать
market[market.isna().any(axis=1)]


market = market.dropna()


market.sample(3)














# проведем корреляционный анализ признаков
market_cor = market.drop(columns=['id'])
correlation_matrix = market_cor.phik_matrix()

# визуализируем
plt.figure(figsize=(18, 12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Матрица корреляции (phik)')
plt.show()

# ЗЫ: знаю, что лучше писать руками признаки, но мне так нравится больше + я проверяю, чтобы все было нормально


# выведем корреляцию покупательской активности
corr = correlation_matrix['покупательская_активность'].sort_values(ascending=False)

display("Сильня корреляция с покупательская_активность:")
display(corr[corr >= 0.5])

display("Слабая корреляция с покупательская_активность:")
display(corr[corr < 0.5])


# проверим зависимость VIF
X_test = market.drop(columns=['id', 'покупательская_активность'])

X_test = pd.get_dummies(X_test, drop_first=True)
X_test = X_test.assign(const=1)

for col in X_test.select_dtypes(include='bool').columns:
    X_test[col] = X_test[col].astype(int)

X_test = X_test.dropna()

# Пересчитываем VIF
vif_data = pd.DataFrame()
vif_data["Признаки"] = X_test.columns
vif_data["VIF"] = [variance_inflation_factor(X_test.values, i) for i in range(X_test.shape[1])]

# Удаляем константу из результатов
vif_data = vif_data[vif_data["Признаки"] != "const"]
display(vif_data)














market.sample(2)


active_normal = market[market['покупательская_активность'] == 'Прежний уровень']
active_bad = market[market['покупательская_активность'] == 'Снизилась']


target_1 = 'выручка_текущий_месяц'
target_2 = 'выручка_предыдущий_месяц'
display(format_display('Прежняя активность'))
plot_density_heatmap(active_normal, target_1, target_2)
display(format_display('Снизилась'))
plot_density_heatmap(active_bad, target_1, target_2)





target_1 = 'выручка_текущий_месяц'
target_2 = 'выручка_препредыдущий_месяц'
display(format_display('Прежняя активность'))
plot_density_heatmap(active_normal, target_1, target_2)
display(format_display('Снизилась'))
plot_density_heatmap(active_bad, target_1, target_2)








target_1 = 'выручка_текущий_месяц'
target_2 = 'маркет_актив_6_мес'
display(format_display('Прежняя активность'))
plot_density_heatmap(active_normal, target_1, target_2)
display(format_display('Снизилась'))
plot_density_heatmap(active_bad, target_1, target_2)





plt.figure(figsize=(10, 6))
plt.scatter(market['id'], market['акционные_покупки'], c=market['id'], alpha=0.7)
plt.colorbar(label='ID')
plt.title('Диаграмма рассеяния между ID и акционные_покупки')
plt.xlabel('id')
plt.ylabel('Акционные_покупки')
plt.grid()
plt.show()


plt.figure(figsize=(10, 6))
plt.scatter(market['id'], market['покупательская_активность'], c=market['id'], alpha=0.7)
plt.colorbar(label='ID')
plt.title('Диаграмма рассеяния между ID и покупательская_активность')
plt.xlabel('id')
plt.ylabel('покупательская_активность')
plt.grid()
plt.show()





plt.figure(figsize=(10, 6))
plt.scatter((market['выручка_текущий_месяц'] - market['выручка_предыдущий_месяц']), market['покупательская_активность'], c=market['id'], alpha=0.7)
plt.colorbar(label='ID')
plt.title('Диаграмма рассеяния между ID и разницой выручки')
plt.xlabel('Разница выручки')
plt.ylabel('покупательская_активность')
plt.grid()
plt.show()








# сделаем таргет 0 / 1
# market['покупательская_активность'] = market['покупательская_активность'].apply(lambda x: 1 if x=='Снизилась' else 0)
# market['покупательская_активность'] = market['покупательская_активность'].astype(int) 

le = LabelEncoder()
market['покупательская_активность'] = le.fit_transform(market['покупательская_активность'])
display(market.sample(2))


# и сделаем категорию покупок по акции
market['акционные_покупки_категория'] = market['акционные_покупки'].apply(lambda x: 'Часто' if x>= 0.5 else 'Редко')


market = market.set_index('id')


market.sample(3)


# зафиксируем константы
RANDOM_STATE = 20
TEST_SIZE = 0.2


# разделим на выборки
X_train, X_test, y_train, y_test = train_test_split(
    market.drop(['покупательская_активность'], axis=1),
    market['покупательская_активность'],
    test_size = TEST_SIZE, 
    random_state = RANDOM_STATE,
    stratify = market['покупательская_активность'])


X_train.shape, X_test.shape


# создаём списки с названиями признаков
# ohe_columns
ohe_columns = get_ohe_columns(X_train)
ohe_columns.append('популярная_категория')

# ord_columns
ord_columns = get_ord_columns(X_train)
ord_columns.append('акционные_покупки_категория')
ord_columns.remove('популярная_категория')

# num_columns
num_columns = get_num_columns(X_train)
num_columns.remove('акционные_покупки')


display("Признаки для OHE:", ohe_columns)
display("Признаки для Ordinal:", ord_columns)
display("Количественные признаки:", num_columns)





# создаём пайплайн для подготовки признаков из списка ohe_columns: заполнение пропусков и OHE-кодирование
# SimpleImputer + OHE
ohe_pipe = Pipeline(
    [
        ('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
        ('ohe', OneHotEncoder(drop='first', handle_unknown='error'))
    ]
)


# создаём пайплайн для подготовки признаков из списка ord_columns: заполнение пропусков и Ordinal-кодирование
# SimpleImputer + OE
ord_pipe = Pipeline(
    [
        ('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
        ('ord',  OrdinalEncoder(
                   categories=[
                       ['Редко','Часто'],
                   ], 
                   handle_unknown='use_encoded_value', unknown_value=np.nan
               )
           ),
        ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))
    ]
)


# num_pipeline = Pipeline(steps=[
#     ('simpleImputer', SimpleImputer(strategy='mean')),
#     ('scaler', MinMaxScaler())
# ])


num_pipeline = ColumnTransformer(
    transformers=[
        ('minmax', Pipeline(steps=[
            ('simpleImputer', SimpleImputer(strategy='mean')),
            ('scaler', MinMaxScaler())
        ]), num_columns),

        ('standard', Pipeline(steps=[
            ('simpleImputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), num_columns),

        ('robust', Pipeline(steps=[
            ('simpleImputer', SimpleImputer(strategy='mean')),
            ('scaler', RobustScaler())
        ]), num_columns)
    ]
)


# создаём общий пайплайн для подготовки данных
data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('ord', ord_pipe, ord_columns),
     ('num', num_pipeline, num_columns)
    ], 
    remainder='passthrough'
)


for name, transformer, columns in data_preprocessor.transformers:
    display(f"Transformer: {name}")
    display(f"Columns: {columns}")
    display(f"Pipeline: {transformer}")


scoring_metrics = {
    'roc_auc': 'roc_auc',
    'accuracy': 'accuracy',
    'f1': 'f1'
}





optuna.logging.set_verbosity(optuna.logging.ERROR)
CLASS_WEIGHT = 'balanced'
WEIGHTS_FOR_KNEIGHBORS = 'distance'

result = []
used_params = set()  # Множество для хранения использованных параметров

def objective(trial):
    model_choice = trial.suggest_categorical('model', ['DecisionTree', 'KNeighbors', 'LogisticRegression', 'SVC'])
    
    if model_choice == 'DecisionTree':
        max_depth = trial.suggest_int('max_depth', 2, 15)
        max_features = trial.suggest_int('max_features', 2, 15)
        min_samples_split = trial.suggest_int('min_samples_split', 2, 15)
        params_tuple = (model_choice, max_depth, max_features, min_samples_split)
    
    elif model_choice == 'KNeighbors':
        n_neighbors = trial.suggest_int('n_neighbors', 2, 50)
        params_tuple = (model_choice, n_neighbors)
    
    elif model_choice == 'LogisticRegression':
        C = trial.suggest_float('C', 0.01, 1000.0, log=True)
        penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])
        params_tuple = (model_choice, C, penalty)
    
    elif model_choice == 'SVC':
        degree = trial.suggest_int('degree', 2, 15)
        C = trial.suggest_float('C', 0.01, 1000.0, log=True)
        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])
        params_tuple = (model_choice, degree, C, gamma)

    if params_tuple in used_params:
        return 0
    used_params.add(params_tuple)

    if model_choice == 'DecisionTree':
        model = DecisionTreeClassifier(max_depth=max_depth,
                                       max_features=max_features, 
                                       min_samples_split=min_samples_split,
                                       random_state=RANDOM_STATE,
                                       class_weight=CLASS_WEIGHT)
    
    elif model_choice == 'KNeighbors':
        model = KNeighborsClassifier(n_neighbors=n_neighbors,
                                    weights=WEIGHTS_FOR_KNEIGHBORS)
    
    elif model_choice == 'LogisticRegression':
        model = LogisticRegression(C=C,
                                   random_state=RANDOM_STATE,
                                   solver='liblinear',
                                   penalty=penalty,
                                   class_weight=CLASS_WEIGHT)
    
    elif model_choice == 'SVC':
        model = SVC(degree=degree,
                    C=C,
                    gamma=gamma,
                    random_state=RANDOM_STATE, 
                    probability=True,
                    class_weight=CLASS_WEIGHT)

    pipe_final = Pipeline([
        ('preprocessor', data_preprocessor),
        ('models', model)
    ])
    
    scoring_metric = trial.suggest_categorical('scoring_metric', ['roc_auc', 'accuracy', 'f1'])
    scores = cross_val_score(pipe_final, X_train, y_train, cv=5, scoring=scoring_metric)
    mean_score = scores.mean()
    
    pipe_final.fit(X_train, y_train)
    y_pred = pipe_final.predict(X_test)
    
    if scoring_metric == 'roc_auc':
        test_score = roc_auc_score(y_test, y_pred)
    elif scoring_metric == 'accuracy':
        test_score = accuracy_score(y_test, y_pred)
    elif scoring_metric == 'f1':
        test_score = f1_score(y_test, y_pred)

    result.append({
        'model': model_choice,
        'pipeline': pipe_final,
        'max_depth': trial.params.get('max_depth', None),
        'max_features': trial.params.get('max_features', None),
        'min_samples_split': trial.params.get('min_samples_split', None),
        'n_neighbors': trial.params.get('n_neighbors', None),
        'C': trial.params.get('C', None),
        'degree': trial.params.get('degree', None),
        'gamma': trial.params.get('gamma', None),
        'scoring_metric_train': mean_score,
        'scoring_metric_test': test_score
    })

    return mean_score

study = optuna.create_study(direction='maximize')

n = 50  # 50
for _ in tqdm(range(n), desc="Trials"):
    study.optimize(objective, n_trials=n)

best_params = study.best_params
best_metrics = result[study.best_trial.number]

final_output = {**best_params, **best_metrics}






# display(final_output)
best_params_df = pd.DataFrame(list(final_output.items()), columns=['Параметры', 'Значение'])
display(format_display("Лучшая модель и параметры"))
filtered_df = best_params_df[best_params_df['Значение'].notna()]
display(filtered_df)














# display(result)

result_df = pd.DataFrame(result)
result_df = result_df.sort_values(by=['scoring_metric_train', 'scoring_metric_test'], ascending=False)
display(result_df.head(10))
display(result_df.info())


model_name = best_params['model']

if model_name == 'SVC':
    model = SVC(
        degree=best_params['degree'],
        C=best_params['C'],
        gamma=best_params['gamma'],
        probability=True,
        random_state=RANDOM_STATE
    )
elif model_name == 'KNeighbors':
    model = KNeighborsClassifier(
        n_neighbors=best_params['n_neighbors']
    )
elif model_name == 'LogisticRegression':
    model = LogisticRegression(
        C=best_params['C'],
        solver='liblinear',
        penalty='l1',
        random_state=RANDOM_STATE
    )
elif model_name == 'DecisionTree':
    model = DecisionTreeClassifier(
        max_depth=best_params['max_depth'],
        max_features=best_params['max_features'],
        min_samples_split=best_params['min_samples_split'],
        random_state=RANDOM_STATE
    )

pipe_final = Pipeline([
    ('preprocessor', data_preprocessor),
    ('models', model)
])
    
X_train_new = pipe_final.named_steps['preprocessor'].fit_transform(X_train)
feature_names = pipe_final.named_steps['preprocessor'].get_feature_names_out()
X_train_new = pd.DataFrame(X_train_new, columns=feature_names)

model.fit(X_train_new, y_train)

K = 5
background_data = shap.sample(X_train_new, K)

if model_name == 'SVC':
    explainer = shap.KernelExplainer(model.decision_function, background_data)
elif model_name == 'KNeighbors':
    explainer = shap.KernelExplainer(model.predict_proba, background_data)
elif model_name == 'LogisticRegression':
    explainer = shap.LinearExplainer(model, background_data)
elif model_name == 'DecisionTree':
    explainer = shap.TreeExplainer(model)

X_test_new = pipe_final.named_steps['preprocessor'].transform(X_test)
X_test_new = pd.DataFrame(X_test_new, columns=feature_names)

shap_values = explainer(X_test_new)


display(X_test_new.shape)
display(shap_values.shape)
if X_test_new.shape != shap_values.shape:
    shap_values = shap_values[:, :, 1]


# Первый график
shap.summary_plot(
    shap_values, 
    X_test_new, 
    plot_type="bar",
    max_display=30, 
    plot_size=(15, 15),
    show=False
)

plt.title('Общая значимость признаков', fontsize=25)
plt.xlabel('SHAP значение', fontsize=20)
plt.ylabel('Признаки', fontsize=20)
plt.show()


# Второй график
shap.summary_plot(
    shap_values, 
    X_test_new, 
    plot_type="dot", 
    max_display=30, 
    plot_size=(15, 15),
    show=False
)

plt.title('Важность признаков', fontsize=25)
plt.xlabel('SHAP значение', fontsize=20)
plt.ylabel('Признаки', fontsize=20)
plt.show()





if X_test_new.shape != shap_values.shape:
    shap_values = explainer(X_test_new)

shap_values_array = shap_values.values
shap_values_mean = np.abs(shap_values_array).mean(axis=0)

# создаем DataFrame для удобства
feature_importance = pd.DataFrame({
    'Признак': X_test_new.columns,
    'SHAP значение': shap_values_mean
})

top_features = feature_importance.sort_values(by='SHAP значение', ascending=False).head(10)

display(format_display("Для модели наиболее важны следующие признаки:"))
display(top_features)











# подготовим вероятности снижения покупательской активности
y_test_proba = model.predict_proba(X_test_new)[:, 1]
y_train_proba = model.predict_proba(X_train_new)[:, 1]


# заполним данные в общую базу
X_test['вероятность_снижения'] = y_test_proba
X_train['вероятность_снижения'] = y_train_proba
market = pd.concat([X_test, X_train])


market.sample(2)


# и еще у нас осталась неиспользованная база money
money = money.set_index('id')
market = market.join(money)
market.sample(2)


fig = plt.figure(figsize=(15,15))
sns.scatterplot(data=market, y='прибыль', x='вероятность_снижения')
plt.xlabel('Вероятность снижения активности')
plt.ylabel('Прибыль')
plt.title('Зависимость вероятности снижения активности от выручки')
plt.show()








plot_scatter_with_categories(market)











market['сегмент'] = market.apply( lambda row: 'Таргет' if row['вероятность_снижения'] > 0.75 \
    and row['акционные_покупки_категория']=='Часто' else 'Другие', axis=1)


market.head()


plot_categorical_columns(market[['сегмент']])





# Фильтруем данные
filtered_data = market[market['сегмент'] == 'Таргет']


plot_categorical_columns(filtered_data, 'тип_сервиса')





plot_categorical_columns(filtered_data, 'разрешить_сообщать')





plot_categorical_columns(filtered_data, 'популярная_категория')





histogram(market, 'маркет_актив_6_мес', 'сегмент')





histogram(market, 'страниц_за_визит', 'сегмент')





histogram(market, 'маркет_актив_тек_мес', 'сегмент')





histogram(market, 'длительность', 'сегмент')





histogram(market, 'акционные_покупки', 'сегмент')





histogram(market, 'средний_просмотр_категорий_за_визит', 'сегмент')


histogram(market, 'неоплаченные_продукты_штук_квартал', 'сегмент')


histogram(market, 'ошибка_сервиса', 'сегмент')


histogram(market, 'страниц_за_визит', 'сегмент')


histogram(market, 'выручка_предыдущий_месяц', 'сегмент')


histogram(market, 'выручка_препредыдущий_месяц', 'сегмент')


histogram(market, 'выручка_текущий_месяц', 'сегмент')


histogram(market, 'минут_предыдущий_месяц', 'сегмент')


histogram(market, 'минут_текущий_месяц', 'сегмент')
























